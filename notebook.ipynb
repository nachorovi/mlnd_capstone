{
  "cells": [
    {
      "metadata": {
        "_uuid": "815b9b413850530451e01206ddce063f0e7c53c3"
      },
      "cell_type": "markdown",
      "source": "Questions:\n* When handling categorical features. Do I need to use pd.factorize? see  https://www.kaggle.com/shivamb/homecreditrisk-extensive-eda-baseline-0-772. I'm just using get_dummies() directly.\nA: You can use both, you have to what works best for your dataset. But both can work\n* When using simple imputer, I lose the column names. It turns the pd.DataFrame into a np.ndarray. Is this the right process, or are there other ways to handle missing data, NaN, Inf, numbers too big for INT32, etc?\nA: Consider 'pd.fillna()'. https://pandas.pydata.org/pandas-docs/stable/missing_data.html\n* Should I do one-hot encoding before or after merging the different datasets (ex, application_train & previous_application)?\nA: Merger, one-hot encoding fitting the model\n* For previous_application after I run the simple imputer I don't have any NaN in the np ndarray, but when I turn it into a pandas DataFrame it has 11109336. I need to turn it into a pandas DataFrame to merge it with application_train. What can I do?\nA: Question is not clear"
    },
    {
      "metadata": {
        "_uuid": "a50ca0e6adc3396caaee0109fd82fee783d1073d"
      },
      "cell_type": "markdown",
      "source": "June 7 through June 17\nFuture steps - Must do:\n* Using the other data files (Feature Engineering, Create flat dataset)\n* Data visualization\n* Normalizing data\n* Handling skewed data\nReport:\n* Preliminary results\n\nJune 18 through July 1\nReport:\n* WEEK1: I should have a skeleton of all the areas of the report to share with mentor.\n* WEEK2: I should have a first full version of the report to share with mentor and ready to submit\nFuture steps - Nice to do:\n* CREATE A scikit-learn Pipeline\n* MAKE AN ENSEMBLE METHOD OF MULTIPLE MODELS\n* Measuring model/learner performance using other evaluation metric (LR+, f1, r2)?\n* Hyperparameter tunening (different values until I find a max, use higher percentage of training & validation data)\n* Review Handling missing or invalid data, other better methods?\n\nDONE:\n* Handling missing or invalid data\n* Split train & validation set\n* Hyperparameter tunening\n* Measuring model/learner performance using evaluation metric (auc_roc_score)"
    },
    {
      "metadata": {
        "_uuid": "d355f28e4f031ceb02bb7223ae79f96353cff7c8"
      },
      "cell_type": "markdown",
      "source": "Preliminary results:\n- Categorical values turned into numerical features with one-hot encoding scheme\n- Fill missing or wrong values\n- RFR Default values, only application training data, score: 0.591\n- RFR Best values with GridSearchCV, only application training data, score: 0.722 <- 0.62\n- RFR Best values with GridSearchCV, ALL available data, score: ?\n- ???? Best values with GridSearchCV, ALL available data, score: ?\n- Normalizing Data\n- Unskewing Data\n\nAbbreviations:\nRFR: Random Forest Regressor\n????: Another regressor"
    },
    {
      "metadata": {
        "_uuid": "9615588ff15ee992f488696fa0a5fa1394bb2707"
      },
      "cell_type": "markdown",
      "source": "# **Sections:**\n[1. Import libraries & support functions](#import)  \n[2. Dataset preparation](#data_prep)  \n[3. Exploratory Data Analysis (EDA)](#eda)  \n&nbsp; [3.1 Application Train](#eda_app_train)  \n[4. Feature Engineering](#feat_eng)  \n&nbsp; [4.1 Create a Flat Dataset](#flat_dataset)  \n&nbsp; [4.2 Handle Skewed Continuous Data](#4.1)  \n&nbsp; [4.3 Normalize Continuous Data](#4.2)  \n&nbsp; [4.4 Handle Categorical Variables](#4.3)  \n&nbsp; [4.5 Handle Missing Data](#4.4)  \n&nbsp;&nbsp; [4.5.1 Previous Applications](#4.5)  \n[5. Split Data into Training and Validation](#5)  \n[6. Hyperparameter Tuning](#6)  \n[7. Model Fitting & Prediction](#7)  "
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSVfile I/O (e.g. pd.read_csv)\nimport os\nfrom plotly.offline import init_notebook_mode, iplot\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom plotly import tools\n# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\nfrom sklearn.ensemble import RandomForestRegressor\n# Add evaluation metric to measure the model's performance\n# Regression metrics available:\n# http://scikit-learn.org/stable/modules/classes.html#regression-metrics\n# http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\n# http://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc\n# http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html\n# Cannot use sklearn.metrics.accuracy_score as it is a Classification metric\nfrom sklearn.metrics import make_scorer, r2_score, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom time import time\nfrom IPython.display import display # Allows the use of display() for DataFrames",
      "execution_count": 83,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "ffc696cb463655b657ded7192c08a3ced678eab1"
      },
      "cell_type": "code",
      "source": "# Enable Debugging while I test things\ndebugging = False # True or False",
      "execution_count": 84,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b0dc45408902e93d5cc4e3b7825abe8d9d5d5914"
      },
      "cell_type": "code",
      "source": "# Support functions\n'''\ndef bar_hor(df, col, title, color, w=None, h=None, lm=0, limit=100, return_trace=False, rev=False, xlb = False):\n    cnt_srs = df[col].value_counts()\n    yy = cnt_srs.head(limit).index[::-1] \n    xx = cnt_srs.head(limit).values[::-1] \n    if rev:\n        yy = cnt_srs.tail(limit).index[::-1] \n        xx = cnt_srs.tail(limit).values[::-1] \n    if xlb:\n        trace = go.Bar(y=xlb, x=xx, orientation = 'h', marker=dict(color=color))\n    else:\n        trace = go.Bar(y=yy, x=xx, orientation = 'h', marker=dict(color=color))\n    if return_trace:\n        return trace \n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n\ndef gp(col, title):\n    df1 = data_train[data_train[\"TARGET\"] == 1]\n    df0 = data_train[data_train[\"TARGET\"] == 0]\n    a1 = df1[col].value_counts()\n    b1 = df0[col].value_counts()\n    \n    total = dict(data_train[col].value_counts())\n    x0 = a1.index\n    x1 = b1.index\n    \n    y0 = [float(x)*100 / total[x0[i]] for i,x in enumerate(a1.values)]\n    y1 = [float(x)*100 / total[x1[i]] for i,x in enumerate(b1.values)]\n\n    trace1 = go.Bar(x=a1.index, y=y0, name='Target : 1', marker=dict(color=\"#96D38C\"))\n    trace2 = go.Bar(x=b1.index, y=y1, name='Target : 0', marker=dict(color=\"#FEBFB3\"))\n    return trace1, trace2 \n'''",
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 85,
          "data": {
            "text/plain": "'\\ndef bar_hor(df, col, title, color, w=None, h=None, lm=0, limit=100, return_trace=False, rev=False, xlb = False):\\n    cnt_srs = df[col].value_counts()\\n    yy = cnt_srs.head(limit).index[::-1] \\n    xx = cnt_srs.head(limit).values[::-1] \\n    if rev:\\n        yy = cnt_srs.tail(limit).index[::-1] \\n        xx = cnt_srs.tail(limit).values[::-1] \\n    if xlb:\\n        trace = go.Bar(y=xlb, x=xx, orientation = \\'h\\', marker=dict(color=color))\\n    else:\\n        trace = go.Bar(y=yy, x=xx, orientation = \\'h\\', marker=dict(color=color))\\n    if return_trace:\\n        return trace \\n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\\n    data = [trace]\\n    fig = go.Figure(data=data, layout=layout)\\n    iplot(fig)\\n\\ndef gp(col, title):\\n    df1 = data_train[data_train[\"TARGET\"] == 1]\\n    df0 = data_train[data_train[\"TARGET\"] == 0]\\n    a1 = df1[col].value_counts()\\n    b1 = df0[col].value_counts()\\n    \\n    total = dict(data_train[col].value_counts())\\n    x0 = a1.index\\n    x1 = b1.index\\n    \\n    y0 = [float(x)*100 / total[x0[i]] for i,x in enumerate(a1.values)]\\n    y1 = [float(x)*100 / total[x1[i]] for i,x in enumerate(b1.values)]\\n\\n    trace1 = go.Bar(x=a1.index, y=y0, name=\\'Target : 1\\', marker=dict(color=\"#96D38C\"))\\n    trace2 = go.Bar(x=b1.index, y=y1, name=\\'Target : 0\\', marker=dict(color=\"#FEBFB3\"))\\n    return trace1, trace2 \\n'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "_kg_hide-input": false
      },
      "cell_type": "code",
      "source": "# List available data files\n#print(os.listdir(\"../input\"))\nprint(\"Loading data files...\")\n# Load the application training dataset application_train.csv\ndata_train = pd.read_csv(\"../input/application_train.csv\")\n# Load the application testing dataset application_test.csv\ndata_test = pd.read_csv(\"../input/application_test.csv\")\n# Load the previous applications dataset\nprev_app = pd.read_csv(\"../input/previous_application.csv\")\nprint(\"Finished loading data files\")",
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Loading data files...\nFinished loading data files\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "10bd1b898552f438389d8bfe962181528d4aa344"
      },
      "cell_type": "code",
      "source": "'''\n# Display the first 5 records of application_train.csv\ndisplay(data_train.head())\n# Display the first 5 records of application_test.csv\ndisplay(data_test.head())\n# See the first 5 rows of the dataframe\ndisplay(prev_app.head())\n\n# DataFrame statistics summary for selected columns\ndata_train[[\"AMT_INCOME_TOTAL\", \"AMT_CREDIT\", \"AMT_ANNUITY\", \"AMT_GOODS_PRICE\"]].describe()\n#print(len(data_train.columns))\n'''",
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 87,
          "data": {
            "text/plain": "'\\n# Display the first 5 records of application_train.csv\\ndisplay(data_train.head())\\n# Display the first 5 records of application_test.csv\\ndisplay(data_test.head())\\n# See the first 5 rows of the dataframe\\ndisplay(prev_app.head())\\n\\n# DataFrame statistics summary for selected columns\\ndata_train[[\"AMT_INCOME_TOTAL\", \"AMT_CREDIT\", \"AMT_ANNUITY\", \"AMT_GOODS_PRICE\"]].describe()\\n#print(len(data_train.columns))\\n'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3fe5a388f860fb1ebdc3698221db4e0c66697726"
      },
      "cell_type": "code",
      "source": "# Total number of records\nprint(\"Total number of records in the train dataset: {:,}\".format(len(data_train)))\n# Total number of features. Excluding the load ids (SK_ID_CURR) and the target variable (TARGET).\nprint(\"Total number of features in the train dataset: {}\".format(data_train.shape[1] - 2))\n\n# Total number of records\nprint(\"Total number of records in the test dataset: {:,}\".format(len(data_test)))\n# Total number of features. Excluding the load ids (SK_ID_CURR). There is NO target variable (TARGET) in the test dataset.\nprint(\"Total number of features in the test dataset: {}\".format(data_test.shape[1] - 1))\n\n# Total number of features. Excluding the current loan ids (SK_ID_CURR) and the previous loan ids (SK_ID_PREV)\nprint(\"Total number of features of previous applications, before one-hot encoding: {}\".format(prev_app.shape[1] - 2))\nprint(\"Previous applications has {:,} samples\".format(prev_app.shape[0]))\n# Check if therer is any NaN value in the dataset\nprint(\"Total number of NaN in the dataframe: {:,}\".format(prev_app.isnull().sum().sum())) # TODO: REMOVE",
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Total number of records in the train dataset: 307,511\nTotal number of features in the train dataset: 120\nTotal number of records in the test dataset: 48,744\nTotal number of features in the test dataset: 120\nTotal number of features of previous applications, before one-hot encoding: 35\nPrevious applications has 1,670,214 samples\nTotal number of NaN in the dataframe: 11,109,336\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ee2f2a92f145a72cccd670372cd2cd6fefd0dca"
      },
      "cell_type": "code",
      "source": "# GRAPHS ARE NOT DISPLAYING\n'''\n# Target Variable Distribution \nbar_hor(data_train, \"TARGET\", \"Distribution of Target Variable\" , [\"#44ff54\", '#ff4444'], h=350, w=600, lm=200, xlb = ['Target : 1','Target : 0'])\n\ntr0 = bar_hor(data_train, \"CODE_GENDER\", \"Distribution of CODE_GENDER Variable\" ,\"#f975ae\", w=700, lm=100, return_trace= True)\ntr1, tr2 = gp('CODE_GENDER', 'Distribution of Target with Applicant Gender')\n\nfig = tools.make_subplots(rows=1, cols=3, print_grid=False, subplot_titles = [\"Gender Distribution\" , \"Gender, Target=1\" ,\"Gender, Target=0\"])\nfig.append_trace(tr0, 1, 1);\nfig.append_trace(tr1, 1, 2);\nfig.append_trace(tr2, 1, 3);\nfig['layout'].update(height=350, showlegend=False, margin=dict(l=50));\niplot(fig);\n'''",
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 89,
          "data": {
            "text/plain": "'\\n# Target Variable Distribution \\nbar_hor(data_train, \"TARGET\", \"Distribution of Target Variable\" , [\"#44ff54\", \\'#ff4444\\'], h=350, w=600, lm=200, xlb = [\\'Target : 1\\',\\'Target : 0\\'])\\n\\ntr0 = bar_hor(data_train, \"CODE_GENDER\", \"Distribution of CODE_GENDER Variable\" ,\"#f975ae\", w=700, lm=100, return_trace= True)\\ntr1, tr2 = gp(\\'CODE_GENDER\\', \\'Distribution of Target with Applicant Gender\\')\\n\\nfig = tools.make_subplots(rows=1, cols=3, print_grid=False, subplot_titles = [\"Gender Distribution\" , \"Gender, Target=1\" ,\"Gender, Target=0\"])\\nfig.append_trace(tr0, 1, 1);\\nfig.append_trace(tr1, 1, 2);\\nfig.append_trace(tr2, 1, 3);\\nfig[\\'layout\\'].update(height=350, showlegend=False, margin=dict(l=50));\\niplot(fig);\\n'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "6b45c23eb5bd269ec30f30d863cd80a7bb35d9dd"
      },
      "cell_type": "markdown",
      "source": "## <a id=\"4.1\">4.1 Feature Engineering - Create a Flat Dataset</a>"
    },
    {
      "metadata": {
        "_uuid": "cec5ec08de391840245c93255c6b94d934f4bc84"
      },
      "cell_type": "markdown",
      "source": "### <a id=\"4.1.1\">4.1.1 Feature Engineering - Create a Flat Dataset - Previous Applications</a>"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "82d20eb9e4b4ad9db36de0a6bce350c93b8c9b66"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "39dcd898ed95bd5c6a44d4e0e001b61622b7a89c"
      },
      "cell_type": "code",
      "source": "'''\n# Check if therer is any NaN value in the pd dataframe df\ndf.isnull().sum().sum()\n\n# Check if therer is any NaN value in the np nparray np_array\nnp.argwhere(np.isnan(np_array))\n'''",
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 90,
          "data": {
            "text/plain": "'\\n# Check if therer is any NaN value in the pd dataframe df\\ndf.isnull().sum().sum()\\n\\n# Check if therer is any NaN value in the np nparray np_array\\nnp.argwhere(np.isnan(np_array))\\n'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f9700bae328b0424b3830915ab13f584ab6a5f83"
      },
      "cell_type": "code",
      "source": "if debugging:\n    display(prev_app.shape)\n    display(prev_app.head())\n    display(prev_app[prev_app['SK_ID_CURR'] == 271877])",
      "execution_count": 91,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4c30d45c27ccf0e7ae53aa1fc7a0eb0cc7dd881f"
      },
      "cell_type": "code",
      "source": "'''\n#### Q: WHY IS THIS NEEDED???\n# count the number of previous applications for a given 'SK_ID_CURR'\nprev_app_count = prev_app[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nprev_app['SK_ID_PREV'] = prev_app['SK_ID_CURR'].map(prev_app_count['SK_ID_PREV'])\n'''",
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 92,
          "data": {
            "text/plain": "\"\\n#### Q: WHY IS THIS NEEDED???\\n# count the number of previous applications for a given 'SK_ID_CURR'\\nprev_app_count = prev_app[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\\nprev_app['SK_ID_PREV'] = prev_app['SK_ID_CURR'].map(prev_app_count['SK_ID_PREV'])\\n\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "896ba6098d6ad855c36156bbd3e58440f92fef04"
      },
      "cell_type": "code",
      "source": "if debugging:\n    display(prev_apps_count.head())",
      "execution_count": 93,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "96322db15c40149722c42ca521391ed6ebd134b9"
      },
      "cell_type": "code",
      "source": "if debugging:\n    display(data_train.shape)\n    display(data_train.head())",
      "execution_count": 94,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a08b4e3825d6ddcf324d30d35ceca9c01207b4c9"
      },
      "cell_type": "code",
      "source": "### Q: ALL THE CATEGORICAL FEATURES ARE LOST BY DOING THE MEAN\n### TODO: ALL THE CATEGORICAL FEATURES ARE LOST BY DOING THE MEAN\n## Average values for all other features in previous applications\nprev_app_avg = prev_app.groupby('SK_ID_CURR').mean()\nprev_app_avg.columns = ['p_' + col for col in prev_app_avg.columns]\ndata_train = data_train.merge(right=prev_app_avg.reset_index(), how='left', on='SK_ID_CURR')",
      "execution_count": 95,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "52b6221768c1ea02f659df44c4852b8542fd2083"
      },
      "cell_type": "code",
      "source": "if debugging:\n    display(prev_app_avg.shape)\n    display(prev_app_avg.head())\n    #display(prev_app_avg[prev_app_avg['SK_ID_CURR'] == 271877].head())\n    display(data_train.shape)\n    display(data_train['p_AMT_ANNUITY'].head())\n    print(\"Total number of NaN in the data dataframe: {:,}\".format(data.isnull().sum().sum()))",
      "execution_count": 96,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c9e7f293121fd26492be2e8d0334f5b6551e624"
      },
      "cell_type": "code",
      "source": "# Transforming skewed continuous features",
      "execution_count": 97,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "10916d50c700ec898123f3b422bb24064f2e4711"
      },
      "cell_type": "code",
      "source": "# Normalizing numerical features\n#from sklearn.preprocessing import MinMaxScaler",
      "execution_count": 98,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "95b3623001c0e8d93e84e3a9e6811264b7db861f"
      },
      "cell_type": "code",
      "source": "# Turn categorical variables into numerical features using the one-hot encoding scheme\n\n# Total number of features. Excluding the load ids (SK_ID_CURR) and the target variable (TARGET)\nprint(\"Total number of features of train data, before one-hot encoding: {}\".format(data_train.shape[1] - 2))\n# Total number of features. Excluding the load ids (SK_ID_CURR). The test dataset has NO target label (TARGET)\nprint(\"Total number of features of test data, before one-hot encoding: {}\".format(data_test.shape[1] - 1))\n\n# One-hot encoding\ndata_train_encoded = pd.get_dummies(data_train)\ndata_test_encoded = pd.get_dummies(data_test)\n\n# Total number of features. Excluding the loan ids (SK_ID_CURR) and the target variable (TARGET)\nprint(\"Total number of features of train data, after one-hot encoding: {}\".format(data_train_encoded.shape[1] - 2))\n# Total number of features. Excluding the loan ids (SK_ID_CURR). The test dataset has NO target label (TARGET)\nprint(\"Total number of features of test data, after one-hot encoding: {}\".format(data_test_encoded.shape[1] - 1))\n\n# New list of features\n#print(list(data_train_encoded.columns))\n\n# Determine what columns are missing\ntrain_list = list(data_train_encoded.columns)\ntest_list = list(data_test_encoded.columns)\ndifference = [e for e in train_list if e not in test_list]\nprint(difference)\n\n# Add those columns to the test set will all zeros\ndata_test_encoded_complete = data_test_encoded\nfor e in difference:\n    if e != 'TARGET':\n        data_test_encoded_complete[e] = 0\n\nprint(\"Total number of features of test data, after one-hot encoding: {}\".format(data_test_encoded_complete.shape[1] - 1))",
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Total number of features of train data, before one-hot encoding: 140\nTotal number of features of test data, before one-hot encoding: 120\nTotal number of features of train data, after one-hot encoding: 264\nTotal number of features of test data, after one-hot encoding: 241\n['TARGET', 'p_SK_ID_PREV', 'p_AMT_ANNUITY', 'p_AMT_APPLICATION', 'p_AMT_CREDIT', 'p_AMT_DOWN_PAYMENT', 'p_AMT_GOODS_PRICE', 'p_HOUR_APPR_PROCESS_START', 'p_NFLAG_LAST_APPL_IN_DAY', 'p_RATE_DOWN_PAYMENT', 'p_RATE_INTEREST_PRIMARY', 'p_RATE_INTEREST_PRIVILEGED', 'p_DAYS_DECISION', 'p_SELLERPLACE_AREA', 'p_CNT_PAYMENT', 'p_DAYS_FIRST_DRAWING', 'p_DAYS_FIRST_DUE', 'p_DAYS_LAST_DUE_1ST_VERSION', 'p_DAYS_LAST_DUE', 'p_DAYS_TERMINATION', 'p_NFLAG_INSURED_ON_APPROVAL', 'CODE_GENDER_XNA', 'NAME_INCOME_TYPE_Maternity leave', 'NAME_FAMILY_STATUS_Unknown']\nTotal number of features of test data, after one-hot encoding: 264\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c29b942e0d37f6e2fc3249da9c88c9bbb2b0a84"
      },
      "cell_type": "code",
      "source": "# Separate the target label from the train dataset. The column we are interested is 'TARGET'. Name it target_train.\ntarget_train = data_train['TARGET']\nprint(\"Training target label has {:,} samples\".format(target_train.shape[0]))\n\n# Remove target label from the train dataset and rename to features_train.\nfeatures_train = data_train_encoded.drop(['TARGET'], axis=1)\n\n# Test data has no taget label 'TARGET' in the dataset\nfeatures_test = data_test_encoded_complete",
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Training target label has 307,511 samples\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "70653fe98cc7a8c16117c897ecc2308223fdbbb0",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Handle missing values\n# Source: https://www.kaggle.com/dansbecker/handling-missing-values\n# http://scikit-learn.org/dev/modules/generated/sklearn.impute.SimpleImputer.html\n\nfrom sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\nfeatures_train = my_imputer.fit_transform(features_train)\n# Use this instead:\n#features_test = my_imputer.transform(features_test)\nfeatures_test = my_imputer.fit_transform(features_test)\n\n# Consider turning features_train & features_test into a pandas DataFrame, instead of being a numpy ndarray\n# prev_app_imputed = pd.DataFrame(my_imputer.fit_transform(prev_app_encoded), columns=prev_app_encoded.columns)",
      "execution_count": 101,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9f0fe582933f1a9086f982e0629a17ae1f9a266",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1de08efe6afcbc5ce61cc3711f9c250d07ec96c6"
      },
      "cell_type": "code",
      "source": "# Shuffle and split the data\n# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\nfrom sklearn.model_selection import train_test_split\n\n# Split the 'features' and 'target label' data into training and validating sets\nX_train, X_val, y_train, y_val = train_test_split(features_train,\n                                                  target_train,\n                                                  test_size=0.2,\n                                                  random_state=42)\n\nprint(\"Original Training set has {:,} samples.\".format(features_train.shape[0]))\nprint(\"After split Training set has {:,} samples.\".format(X_train.shape[0]))\nprint(\"After split Validating set has {:,} samples.\".format(X_val.shape[0]))\nprint(\"Testing set has {:,} samples.\".format(data_test.shape[0]))",
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Original Training set has 307,511 samples.\nAfter split Training set has 246,008 samples.\nAfter split Validating set has 61,503 samples.\nTesting set has 48,744 samples.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "51ba916b1d62b15d1d2fc1315a5f7647903e454f"
      },
      "cell_type": "code",
      "source": "# Run GridSearchCV or fully train an estimator\nrun_mode = 'train_estimator' # 'grid_search' or 'train_estimator'",
      "execution_count": 103,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d3362fd6c2b2cc65deaa4a18ecb49ed7a02d8263",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Run GridSearchCV\nif run_mode == 'grid_search':\n    perc_samples = 0.001\n    print(\"Preparing to run Hyperparameters tunning with GridSearchCV using {0:.2f}% of the training samples\".format(perc_samples * 100))\n    features_train_small = X_train[:int(perc_samples * X_train.shape[0])]\n    target_train_small = y_train[:int(perc_samples * y_train.shape[0])]\n    features_val_small = X_val[:int(perc_samples * X_val.shape[0])]\n    target_val_small = y_val[:int(perc_samples * y_val.shape[0])]\n    #features_test_small = features_test[:int(perc_samples * features_test.shape[0])]\n\n    # Initialize the Estimator (Learner or Regression Model)\n    estimator = RandomForestRegressor(n_jobs=-1,\n                                      random_state=42,\n                                      verbose=0)\n\n    # Determine which Parameters to tune\n    '''\n    Tested so far:\n    parameters = {\n        'n_estimators': [9, 10, 11, 12, 13, 14, 15],\n        'criterion': ['mse', 'mae'],\n        'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7],\n        'max_features': [0.01, 0.1, 0.25, 0.45, 0.5, 0.55, 0.6, 0.75],\n        'min_samples_split': [2, 3, 4, 5],\n        'warm_start': [False, True]\n    }\n    '''\n    parameters = {\n        'n_estimators': [115, 125, 135],\n        'min_samples_leaf': [62, 75, 87],\n        'max_features': [0.18, 0.2, 0.23],\n        'min_samples_split': [2, 3],\n    }\n\n    # Create a scorer to measure hyperparameters performance\n    scorer = make_scorer(roc_auc_score)\n\n    # Create GridSearchCV grid object\n    grid_obj = GridSearchCV(estimator=estimator, \n                            param_grid=parameters, \n                            scoring=scorer)\n\n    # Fit the GridSearchCV grid object with the reduced training dataset and find the best hyperparameters\n    start = time()\n    grid_fit = grid_obj.fit(features_train_small, target_train_small)\n    end = time()\n    grid_fit_time = (end - start) / 60 # Ellapsed time in minutes\n    print(\"GridSearchCV estimator fit time: {0:.2f} minutes\".format((end - start) / 60))\n\n    # Get the best estimator\n    best_est = grid_obj.best_estimator_\n    print(\"Best Estimator: \\n{}\\n\".format(best_est))\n\n    # Get the best score\n    best_score = grid_obj.best_score_\n    print(\"Best Estimator Score: {}\\n\".format(best_score))\n\n    # Get the best parameters\n    best_params = grid_obj.best_params_\n    print(\"Best Hyperparameters that yield the best score: \\n{}\\n\".format(best_params))\n\n    # Make predictions with unoptimized estimator on the validation set\n    pred_val = (estimator.fit(features_train_small, target_train_small)).predict(features_val_small)\n    print(\"Unoptimized Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(target_val_small, pred_val)))\n\n    # Predict with the best estimator on the validation set\n    best_pred_val = best_est.predict(features_val_small)\n    print(\"Optimized Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(target_val_small, best_pred_val)))\n\n    # Predict with the best estimator on the testing set\n    #pred_test = best_est.predict(features_test)",
      "execution_count": 104,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06daaf465dc5e34eab324a1ecd7b3c80183b73ed"
      },
      "cell_type": "code",
      "source": "# Train estimator\n# TODO: rename or remove '_small', it might be '_full' or nothing\nif run_mode == 'train_estimator':\n    # Use the full training and validation datasets to fit the estimator with the best hyperparameters\n    perc_samples = 1\n    print(\"Preparing to train an estimator using {0:.2f}% of the training dataset\".format(perc_samples * 100))\n    features_train_small = X_train[:int(perc_samples * X_train.shape[0])]\n    target_train_small = y_train[:int(perc_samples * y_train.shape[0])]\n    features_val_small = X_val[:int(perc_samples * X_val.shape[0])]\n    target_val_small = y_val[:int(perc_samples * y_val.shape[0])]\n    features_test_small = features_test[:int(perc_samples * features_test.shape[0])]\n\n    # Initialize the Estimator (Learner or Regression Model) with the best hyperparameters\n    '''\n    estimator = RandomForestRegressor(criterion='mae', # default='mse', VERY SLOW\n                                      min_samples_split=2, # default=2\n                                      warm_start=False) # default=False\n    Best Performance:\n    estimator = RandomForestRegressor(n_estimators=125, # default=10\n                                      max_features=0.2, # default='auto'\n                                      min_samples_split=2, # default=2\n                                      min_samples_leaf=75, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=2) # default=0\n                                      ROC_AUC_SCORE:  0.7417496532130599 (% samples: 0.15)\n                                      LEADERBOARD SCORE: 0.722 (NO improvement)\n\n    estimator = RandomForestRegressor(n_estimators=100, # default=10\n                                      max_features=0.2, # default='auto'\n                                      min_samples_split=3, # default=2\n                                      min_samples_leaf=50, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=2) # default=0\n                                      ROC_AUC_SCORE:  0.7286786065442892 (% samples: 0.1)\n                                      LEADERBOARD SCORE: 0.722\n\n    estimator = RandomForestRegressor(n_estimators=12, # default=10\n                                      max_features=0.45, # default='auto'\n                                      min_samples_leaf=2, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=2) # default=0\n                                      LEADERBOARD SCORE: 0.62\n    '''\n    estimator = RandomForestRegressor(n_estimators=125, # default=10\n                                      max_features=0.2, # default='auto'\n                                      min_samples_split=2, # default=2\n                                      min_samples_leaf=75, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=0) # default=0\n\n    # Fit the estimator with the training dataset\n    start = time()\n    estimator.fit(features_train_small, target_train_small)\n    end = time()\n    print(\"Estimator fit time: {0:.2f} minutes\".format((end - start) / 60))\n\n    # Predict with the validation dataset\n    pred_val = estimator.predict(features_val_small)\n    print(\"Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(target_val_small, pred_val)))\n    \n    # Predict using the 'test' dataset for submission\n    pred_test = estimator.predict(features_test_small)\n    #pred_test = estimator.predict(features_test)\n\n    # Prepare prediction for submission\n    submission = pd.DataFrame()\n    # Need to replace data_test_encoded_complete\n    submission['SK_ID_CURR'] = data_test_encoded_complete['SK_ID_CURR'][:int(perc_samples * features_test.shape[0])]\n    submission['TARGET'] = pred_test\n    submission.head()\n    submission.to_csv('RFR.csv', index=False)",
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Preparing to train an estimator using 100.00% of the training dataset\nEstimator fit time: 4.02 minutes\nEstimator prediction score on Validation set: \t0.7542736992401178\n",
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}