{
  "cells": [
    {
      "metadata": {
        "_uuid": "815b9b413850530451e01206ddce063f0e7c53c3"
      },
      "cell_type": "markdown",
      "source": "Questions:\n* I get a better competition score by training my model only with application_train.csv than what I get when I train it will ALL the datasets. Am I merging them correctly?\n* A: \n* Creating Flat Dataset. When I mreged previous_application with application_train using .groupby().mean() it dropped the categorical features! How do I avoid this?\n* A:\n* Is 'roc_auc_score' an appropriate metric? What else could I use? I mreged previous_application with application_train, I got a higher 'roc_auc_score' but a lower submission score.\n* A: \n* When handling categorical features. Do I need to use pd.factorize? see  https://www.kaggle.com/shivamb/homecreditrisk-extensive-eda-baseline-0-772. I'm just using get_dummies() directly.\n* A: You can use both, you have to what works best for your dataset. But both can work\n\nDONE:\n* For previous_application after I run the simple imputer I don't have any NaN in the np ndarray, but when I turn it into a pandas DataFrame it has 11109336. I need to turn it into a pandas DataFrame to merge it with application_train. What can I do?\n* A: Question is not clear\n* When using simple imputer, I lose the column names. It turns the pd.DataFrame into a np.ndarray. Is this the right process, or are there other ways to handle missing data, NaN, Inf, numbers too big for INT32, etc?\n* A: Consider 'pd.fillna()'. https://pandas.pydata.org/pandas-docs/stable/missing_data.html\n* Should I do one-hot encoding before or after merging the different datasets (ex, application_train & previous_application)?\n* A: Merger, one-hot encoding fitting the model.\n"
    },
    {
      "metadata": {
        "_uuid": "a50ca0e6adc3396caaee0109fd82fee783d1073d"
      },
      "cell_type": "markdown",
      "source": "June 7 through June 17\nFuture steps - Must do:\n* Try one-hot encoding on the individual datasets before merging. Create a function. Do fillna after merging. Using the other data files (Feature Engineering, Create flat dataset), categorical features are being dropped.\n* Review Capstone project report requirements.\n* Review Capstone project proposal response.\n* Data visualization\n* Clean Notebook.\n* Add feature importance graph for RFR\n* Normalizing data\n* Handling skewed data\n* When running GridSearchCV then use Train Test Split, but when just training the model don't split the data. Is the competition test higher? Is this what the other kernels do?\n* Try other regressors: GBRT, AdaBoost, SGDC\n* Try other regressor: LGBM - See why performance is low\n* Try using other metrics to evaluate the model's performance to see if they more accureately align with the project score.\n* Add reference to other kernels in mine: https://www.kaggle.com/shivamb/homecreditrisk-extensive-eda-baseline-0-772, https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm/code\nReport:\n* Preliminary results\n\nJune 18 through July 1\nReport:\n* WEEK1: I should have a skeleton of all the areas of the report to share with mentor.\n* WEEK2: I should have a first full version of the report to share with mentor and ready to submit\nFuture steps - Nice to do:\n* Only apply one-hot encoding to columns which are not numeric.\n* Consider doing 'pd.factorize' for one-hot encoding\n* CREATE A scikit-learn Pipeline\n* MAKE AN ENSEMBLE METHOD OF MULTIPLE MODELS\n* Measuring model/learner performance using other evaluation metric (LR+, f1, r2)?\n* Hyperparameter tunening (different values until I find a max, use higher percentage of training & validation data)\n* Review Handling missing or invalid data, other better methods?\n* See if sklearn.model_selection.'KFold' could yield a better result than 'train_test_split'\n\nDONE:\n* Re-train LGBM model with same parameters but app_train data only. Submit. 0.747.\n* Re-train LGBM model with same parameters but ALL data. Submit. 0.745.\n* Re-train model with same parameters but all data. Submit. 0.719.\n* Handle missing and categorical variables for the Flat (ALL) dataset before splitting the data. Explain why. Submit. 0.732.\n* Remove SK_ID_CURR from dataset before fitting the model\n* Add feature importance - For LGBM & RFR (without graph)\n* Try other regressor: LGBM - First pass complete, performance is much lower than example\n* Using the other data files (Feature Engineering, Create flat dataset) first pass complete\n* Handling missing or invalid data\n* Split train & validation set\n* Hyperparameter tunening\n* Measuring model/learner performance using evaluation metric (auc_roc_score)\n* Handle missing data or NaN, try with pd.fillna()"
    },
    {
      "metadata": {
        "_uuid": "d355f28e4f031ceb02bb7223ae79f96353cff7c8"
      },
      "cell_type": "markdown",
      "source": "    Preliminary results:\n- Categorical values turned into numerical features with one-hot encoding scheme\n- Fill missing or wrong values\n- RFR Default values, only application training data, score: 0.591\n- RFR Best values with GridSearchCV, only application training data, score: 0.722 <- 0.62 (surpassing the 0.688 benchmark for Random Forrest)\n- Got a score of 0.732 with the same model parameters, but by merging train & test datasets before doing pre-processing.\n- RFR Best values with GridSearchCV, ALL available data, score: 0.709 & 0.710\n- I'm still don't why with ALL the datasets I get a lower competition score than with just application_train.\n- I'm still not sure why I keep getting a higher roc_auc_score with ALL the datasets than with just application_train, but the competition score is lower.\n- LGBM Best values with GridSearchCV, ALL available data, score: 0.745\n- LGBM Best values with GridSearchCV, app_train available data, score: 0.745\n- I don't know why the score was so low. Based on the original kernel (https://www.kaggle.com/shivamb/homecreditrisk-extensive-eda-baseline-0-772) the score should have been 0.772.\n- Lessons learnt so far:\n- Need and how to create a flat datasat from multiple datafiles. Explain what I did. Do some little research. Other ways to merge the data that doesn't use mean.\n- Feature engineering. More art that science. Explain concept. They only new feature I created is the count of number of records when there are multiple number of records of some typer per unique load id SK_ID_CURR. What other features could be created? How? (from the result of the data analysis & experience)\n- Problem when merging datasets. Categorical values were being dropped. when merging. Solution. Do one-hot encoding on each dataset before merging. Retest on RFR, LGBM with app_train only (shouldn't be needed, I already know whit score) with ALL the NEW dataset, it should have several new features from the merged datasets.\n- Merge train and test datasets together to do the pre-processing. Explain. Makes the mean more meaningful when completing empty values with the column mean. And also makes sure the test dataset has the same features than the train set. Otherwise if the test set is smaller there is a chance that it doesn't have all the categorical features, so when you apply one-hot encoding you'll end up missing features and the prediction will fail because the model and the test dataset sizes don't match.\n- ???? Best values with GridSearchCV, ALL available data, score: ? (PENDING)\n- Handling missing data (Why don't they do it?)\n- Handling Categorical variables, why do they use pd.factorize?\n- Explain why I keep dataset merged before separating train and validation set. The mean used is more relevant, given that the test data set with be fewer samples. The reason to do one-hot encoding all together is because both the training and validation dataset need to have the same features, and doing it togethers ensures this.\n- Normalizing Data (PENDING)\n- Unskewing Data (PENDING)\n\nAbbreviations:\nRFR: Random Forest Regressor\n????: Another regressor"
    },
    {
      "metadata": {
        "_uuid": "9615588ff15ee992f488696fa0a5fa1394bb2707"
      },
      "cell_type": "markdown",
      "source": "# **Sections:**\n[1. Import libraries & support functions](#import)  \n[2. Dataset preparation](#data_prep)  \n[3. Exploratory Data Analysis (EDA)](#eda)  \n&nbsp; [3.1 Application Train](#eda_app_train)  \n[4. Feature Engineering](#feat_eng)  \n&nbsp; [4.1 Create a Flat Dataset](#flat_dataset)  \n&nbsp; [4.2 Handle Skewed Continuous Data](#4.1)  \n&nbsp; [4.3 Normalize Continuous Data](#4.2)  \n&nbsp; [4.4 Handle Categorical Variables](#4.3)  \n&nbsp; [4.5 Handle Missing Data](#4.4)  \n&nbsp;&nbsp; [4.5.1 Previous Applications](#4.5)  \n[5. Split Data into Training and Validation](#5)  \n[6. Hyperparameter Tuning](#6)  \n[7. Model Fitting & Prediction](#7)  "
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSVfile I/O (e.g. pd.read_csv)\nimport os\nfrom plotly.offline import init_notebook_mode, iplot\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom plotly import tools\n# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\nfrom sklearn.ensemble import RandomForestRegressor\n# http://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n# https://github.com/Microsoft/LightGBM\nimport lightgbm as lgb\n# Add evaluation metric to measure the model's performance\n# Regression metrics available:\n# http://scikit-learn.org/stable/modules/classes.html#regression-metrics\n# http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\n# http://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc\n# http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html\n# Cannot use sklearn.metrics.accuracy_score as it is a Classification metric\nfrom sklearn.metrics import make_scorer, r2_score, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom time import time\nfrom IPython.display import display # Allows the use of display() for DataFrames\n# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\nfrom sklearn.model_selection import train_test_split",
      "execution_count": 40,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "ffc696cb463655b657ded7192c08a3ced678eab1"
      },
      "cell_type": "code",
      "source": "# Enable Debugging while I test things\ndebugging = False # True or False",
      "execution_count": 41,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b0dc45408902e93d5cc4e3b7825abe8d9d5d5914"
      },
      "cell_type": "code",
      "source": "# Support functions\n'''\ndef bar_hor(df, col, title, color, w=None, h=None, lm=0, limit=100, return_trace=False, rev=False, xlb = False):\n    cnt_srs = df[col].value_counts()\n    yy = cnt_srs.head(limit).index[::-1] \n    xx = cnt_srs.head(limit).values[::-1] \n    if rev:\n        yy = cnt_srs.tail(limit).index[::-1] \n        xx = cnt_srs.tail(limit).values[::-1] \n    if xlb:\n        trace = go.Bar(y=xlb, x=xx, orientation = 'h', marker=dict(color=color))\n    else:\n        trace = go.Bar(y=yy, x=xx, orientation = 'h', marker=dict(color=color))\n    if return_trace:\n        return trace \n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n\ndef gp(col, title):\n    df1 = data_train[data_train[\"TARGET\"] == 1]\n    df0 = data_train[data_train[\"TARGET\"] == 0]\n    a1 = df1[col].value_counts()\n    b1 = df0[col].value_counts()\n    \n    total = dict(data_train[col].value_counts())\n    x0 = a1.index\n    x1 = b1.index\n    \n    y0 = [float(x)*100 / total[x0[i]] for i,x in enumerate(a1.values)]\n    y1 = [float(x)*100 / total[x1[i]] for i,x in enumerate(b1.values)]\n\n    trace1 = go.Bar(x=a1.index, y=y0, name='Target : 1', marker=dict(color=\"#96D38C\"))\n    trace2 = go.Bar(x=b1.index, y=y1, name='Target : 0', marker=dict(color=\"#FEBFB3\"))\n    return trace1, trace2 \n'''",
      "execution_count": 42,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "_kg_hide-input": false
      },
      "cell_type": "code",
      "source": "# List available data files\n#print(os.listdir(\"../input\"))\nprint(\"Loading data files...\")\n\nstart = time()\n# Load the Point of Sale Cash balance dataset\nposc_bal = pd.read_csv(\"../input/POS_CASH_balance.csv\")\n# Load the Bureau Balance dataset\nbureau_bal = pd.read_csv(\"../input/bureau_balance.csv\")\n# Load the Application Training dataset\napp_train = pd.read_csv(\"../input/application_train.csv\")\n# Load the Previous Applications dataset\nprev_app = pd.read_csv(\"../input/previous_application.csv\")\n# Load the Installements Payments dataset\ninst_pay = pd.read_csv(\"../input/installments_payments.csv\")\n# Load the Credit Card Balance dataset\ncc_bal = pd.read_csv(\"../input/credit_card_balance.csv\")\n# Load the Application Testing dataset\napp_test = pd.read_csv(\"../input/application_test.csv\")\n# Load the Bureau dataset\nbureau = pd.read_csv(\"../input/bureau.csv\")\nend = time()\n\nprint(\"Finished loading data files in {} seconds.\".format(int(round(end - start))))",
      "execution_count": 43,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "10bd1b898552f438389d8bfe962181528d4aa344"
      },
      "cell_type": "code",
      "source": "'''\n# Display the first 5 records of application_train.csv\ndisplay(data_train.head())\n# Display the first 5 records of application_test.csv\ndisplay(data_test.head())\n# See the first 5 rows of the dataframe\ndisplay(prev_app.head())\n\n# DataFrame statistics summary for selected columns\ndata_train[[\"AMT_INCOME_TOTAL\", \"AMT_CREDIT\", \"AMT_ANNUITY\", \"AMT_GOODS_PRICE\"]].describe()\n#print(len(data_train.columns))\n'''",
      "execution_count": 44,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3fe5a388f860fb1ebdc3698221db4e0c66697726"
      },
      "cell_type": "code",
      "source": "'''\n# Total number of records\nprint(\"Total number of records in the train dataset: {:,}\".format(len(data_train)))\n# Total number of features. Excluding the load ids (SK_ID_CURR) and the target variable (TARGET).\nprint(\"Total number of features in the train dataset: {}\".format(data_train.shape[1] - 2))\n\n# Total number of records\nprint(\"Total number of records in the test dataset: {:,}\".format(len(data_test)))\n# Total number of features. Excluding the load ids (SK_ID_CURR). There is NO target variable (TARGET) in the test dataset.\nprint(\"Total number of features in the test dataset: {}\".format(data_test.shape[1] - 1))\n\n# Total number of features. Excluding the current loan ids (SK_ID_CURR) and the previous loan ids (SK_ID_PREV)\nprint(\"Total number of features of previous applications, before one-hot encoding: {}\".format(prev_app.shape[1] - 2))\nprint(\"Previous applications has {:,} samples\".format(prev_app.shape[0]))\n# Check if therer is any NaN value in the dataset\nprint(\"Total number of NaN in the dataframe: {:,}\".format(prev_app.isnull().sum().sum())) # TODO: REMOVE\n'''",
      "execution_count": 45,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ee2f2a92f145a72cccd670372cd2cd6fefd0dca"
      },
      "cell_type": "code",
      "source": "# GRAPHS ARE NOT DISPLAYING\n'''\n# Target Variable Distribution \nbar_hor(data_train, \"TARGET\", \"Distribution of Target Variable\" , [\"#44ff54\", '#ff4444'], h=350, w=600, lm=200, xlb = ['Target : 1','Target : 0'])\n\ntr0 = bar_hor(data_train, \"CODE_GENDER\", \"Distribution of CODE_GENDER Variable\" ,\"#f975ae\", w=700, lm=100, return_trace= True)\ntr1, tr2 = gp('CODE_GENDER', 'Distribution of Target with Applicant Gender')\n\nfig = tools.make_subplots(rows=1, cols=3, print_grid=False, subplot_titles = [\"Gender Distribution\" , \"Gender, Target=1\" ,\"Gender, Target=0\"])\nfig.append_trace(tr0, 1, 1);\nfig.append_trace(tr1, 1, 2);\nfig.append_trace(tr2, 1, 3);\nfig['layout'].update(height=350, showlegend=False, margin=dict(l=50));\niplot(fig);\n'''",
      "execution_count": 46,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "_uuid": "6b45c23eb5bd269ec30f30d863cd80a7bb35d9dd"
      },
      "cell_type": "markdown",
      "source": "## <a id=\"4.1\">4.1 Feature Engineering - Create a Flat Dataset</a>"
    },
    {
      "metadata": {
        "_uuid": "cec5ec08de391840245c93255c6b94d934f4bc84"
      },
      "cell_type": "markdown",
      "source": "### <a id=\"4.1.1\">4.1.1 Feature Engineering - Create a Flat Dataset - Previous Applications</a>"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "1f1b7d135accee867bddc35318ddffdd15b622ee"
      },
      "cell_type": "code",
      "source": "# Keep a copy of the application_train dataset without merging with the rest of the datasets\ndata_train = app_train.copy()\n#data_test = app_test.copy()",
      "execution_count": 47,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "270ef25dfa972d28a962bae4981248fb1c7e3ce3",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Merge Point of Sale Cash Balance dataset\n# Count the number of previous applications for a given 'SK_ID_CURR', and create a new feature\nposc_bal_count = posc_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nposc_bal['POSC_BAL_COUNT'] = posc_bal['SK_ID_CURR'].map(posc_bal_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\nposc_bal = posc_bal.drop(['SK_ID_PREV'], axis=1)\n\n# Average values for all other features in previous applications\nposc_bal_avg = posc_bal.groupby('SK_ID_CURR').mean()\nposc_bal_avg.columns = ['pcb_' + col for col in posc_bal_avg.columns]\ndata_train = data_train.merge(right=posc_bal_avg.reset_index(), how='left', on='SK_ID_CURR')",
      "execution_count": 48,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "379a233789f277ca8630dae8c5d5232f3ff2f793"
      },
      "cell_type": "code",
      "source": "'''# Merge Bureau Balance dataset\n#'SK_ID_BUREAU'\n# Count the number of previous applications for a given 'SK_ID_CURR', and create a new feature\nbureau_bal_count = bureau_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nbureau_bal['bureau_bal_COUNT'] = bureau_bal['SK_ID_CURR'].map(bureau_bal_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\nbureau_bal = bureau_bal.drop(['SK_ID_PREV'], axis=1)\n\n# Average values for all other features in previous applications\nbureau_bal_avg = bureau_bal.groupby('SK_ID_CURR').mean()\nbureau_bal_avg.columns = ['posc_' + col for col in bureau_bal_avg.columns]\ndata_train = data_train.merge(right=bureau_bal_avg.reset_index(), how='left', on='SK_ID_CURR')\n'''",
      "execution_count": 49,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "0936aa22007519ec1df7bb385e4f1a8f6a2b67ff",
        "_kg_hide-input": true,
        "_kg_hide-output": true
      },
      "cell_type": "code",
      "source": "if debugging:\n    display(prev_app.shape)\n    display(prev_app.head())\n    display(prev_app[prev_app['SK_ID_CURR'] == 271877])\n    display(data_train.shape)\n    display(data_train.head())",
      "execution_count": 50,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "73ce63a2eadd2d575ea52b317a55c24acf09951d"
      },
      "cell_type": "code",
      "source": "# Merge Previous Applications dataset\n# Count the number of previous applications for a given 'SK_ID_CURR'\nprev_app_count = prev_app[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nprev_app['PREV_COUNT'] = prev_app['SK_ID_CURR'].map(prev_app_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\nprev_app = prev_app.drop(['SK_ID_PREV'], axis=1)\n\n# Average values for all other features in previous applications\nprev_app_avg = prev_app.groupby('SK_ID_CURR').mean()\nprev_app_avg.columns = ['pa_' + col for col in prev_app_avg.columns]\ndata_train = data_train.merge(right=prev_app_avg.reset_index(), how='left', on='SK_ID_CURR')",
      "execution_count": 51,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "41116d2861dc78d2b9f059563f6fb4098b11cc05",
        "_kg_hide-input": true,
        "_kg_hide-output": true
      },
      "cell_type": "code",
      "source": "if debugging:\n    display(prev_app_count.head())\n    display(prev_app_avg.shape)\n    display(prev_app_avg.head())\n    #display(prev_app_avg[prev_app_avg['SK_ID_CURR'] == 271877].head())\n    display(data_train.shape)\n    display(data_train['p_AMT_ANNUITY'].head())\n    print(\"Total number of NaN in the data dataframe: {:,}\".format(data_train.isnull().sum().sum()))",
      "execution_count": 52,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "848e06dea27f5efb1847debbd5ca9c485ca61908"
      },
      "cell_type": "code",
      "source": "# Merge Installments Payments dataset\n# Count the number of installments payments for a given 'SK_ID_CURR', and create a new feature\ninst_pay_count = inst_pay[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\ninst_pay['INST_PAY_COUNT'] = inst_pay['SK_ID_CURR'].map(inst_pay_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\ninst_pay = inst_pay.drop(['SK_ID_PREV'], axis=1)\n\n## Average values for all other features in previous applications\ninst_pay_avg = inst_pay.groupby('SK_ID_CURR').mean()\ninst_pay_avg.columns = ['ip_' + col for col in inst_pay_avg.columns]\ndata_train = data_train.merge(right=inst_pay_avg.reset_index(), how='left', on='SK_ID_CURR')",
      "execution_count": 53,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "70a191e709a4ba349fea674b039fc40fa9cb1c9e"
      },
      "cell_type": "code",
      "source": "# Merge Credit Card Balance dataset\n# Count the number of previous applications for a given 'SK_ID_CURR', and create a new feature\ncc_bal_count = cc_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\ncc_bal['CC_BAL_COUNT'] = cc_bal['SK_ID_CURR'].map(cc_bal_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\ncc_bal = cc_bal.drop(['SK_ID_PREV'], axis=1)\n\n## Average values for all other features in previous applications\ncc_bal_avg = cc_bal.groupby('SK_ID_CURR').mean()\ncc_bal_avg.columns = ['ccb_' + col for col in cc_bal_avg.columns]\ndata_train = data_train.merge(right=cc_bal_avg.reset_index(), how='left', on='SK_ID_CURR')",
      "execution_count": 54,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "cd03b77d4a01d0d8a7cbb114b1620c83144c628c",
        "_kg_hide-input": true,
        "_kg_hide-output": true
      },
      "cell_type": "code",
      "source": "if debugging:\n    display(bureau.head())\n    display(bureau.shape)",
      "execution_count": 55,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "a85c005254d9976eba6042afd2b06bda36cdea1a"
      },
      "cell_type": "code",
      "source": "# Merge Bureau dataset\n# Count the number of credits registered in the bureau for a given 'SK_ID_CURR', and create a new feature\nbureau_count = bureau[['SK_ID_CURR', 'SK_ID_BUREAU']].groupby('SK_ID_CURR').count()\nbureau['BUREAU_COUNT'] = bureau['SK_ID_CURR'].map(bureau_count['SK_ID_BUREAU'])\n# Remove the 'SK_ID_BUREAU' column from the dataset as it doesn't add value\nbureau = bureau.drop(['SK_ID_BUREAU'], axis=1)\n\n## Average values for all other features in previous applications\nbureau_avg = bureau.groupby('SK_ID_CURR').mean()\nbureau_avg.columns = ['b_' + col for col in bureau_avg.columns]\ndata_train = data_train.merge(right=bureau_avg.reset_index(), how='left', on='SK_ID_CURR')",
      "execution_count": 56,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "b98aece8cd7d5a096157be491f57a1361c9f6206",
        "_kg_hide-input": true,
        "_kg_hide-output": true
      },
      "cell_type": "code",
      "source": "if debugging:\n    display(bureau_avg.head())\n    display(bureau_avg.shape)",
      "execution_count": 57,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c9e7f293121fd26492be2e8d0334f5b6551e624",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Transforming skewed continuous features",
      "execution_count": 58,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "10916d50c700ec898123f3b422bb24064f2e4711"
      },
      "cell_type": "code",
      "source": "# Normalizing numerical features\n#from sklearn.preprocessing import MinMaxScaler",
      "execution_count": 59,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9e96594845b10101ad32583f79665412b3994733",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "data_to_use = 'ALL' # 'ALL' or 'app_train'\nif data_to_use == 'ALL':\n    app_train = data_train.copy()",
      "execution_count": 60,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8673e3d4c80bcae3d653f7d8b87f095381fcc0f2"
      },
      "cell_type": "code",
      "source": "# Just use application_train and application_test\n# Merge training and testing datasets\n# This will help in two ways:\n# When handling categorical variables it will ensure both datasets end up with the same features\n# When handling missing values, if we use the mean to fill in missing values, they will be more representative\napp_train['is_train'] = 1\napp_train['is_test'] = 0\napp_test['is_train'] = 0\napp_test['is_test'] = 1\n#display(app_train.head(3))\n#display(app_test.head(3))\nprint(\"\\nJoining the training(app_train) and testing(app_test) dataset for pre-processing into pandas DataFrame 'data'.\")\ndata = pd.concat([app_train, app_test], axis=0, sort=False)\n# Substract 4 from the features count for the columns 'TARGET', 'SK_ID_CURR', 'is_train', 'is_test' for app_train\n# And substract 3 for app_test, as it doesn't have a 'TARGET' column\nprint(\"app_train has {0:,} samples and {1} features.\".format(app_train.shape[0], app_train.shape[1]-4))\nprint(\"app_test has {0:,} samples and {1} features.\".format(app_test.shape[0], app_test.shape[1]-3))\nprint(\"data has {0:,} samples and {1} features BEFORE one-hot encoding.\".format(data.shape[0], data.shape[1]-4))\nassert(data.shape[0] == app_train.shape[0] + app_test.shape[0])\nassert(data.shape[1] == max(app_train.shape[1], app_test.shape[1]))\n#display(data.head(3))\n#display(data[data['SK_ID_CURR'] == 100001])\n\n# Handle Categorical variables - Turn categorical variables into numerical features using the one-hot encoding scheme\n# http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html\nprint(\"\\nTransforming categorical variables into numerical features using the one-hot encoding scheme.\")\ndata = pd.get_dummies(data)\n# Substract 4 from the features count for the columns 'TARGET', 'SK_ID_CURR', 'is_train', 'is_test'\nprint(\"data has {0:,} samples and {1} features AFTER one-hot encoding.\".format(data.shape[0], data.shape[1]-4))\n#display(data.head(3))\n\n# Handle missing data\n# https://pandas.pydata.org/pandas-docs/stable/missing_data.html#filling-with-a-pandasobject\n# https://www.kaggle.com/dansbecker/handling-missing-values\n# http://scikit-learn.org/dev/modules/generated/sklearn.impute.SimpleImputer.html\nprint(\"\\nFilling NaN values in the dataset using pandas.fillna() using the column mean() value.\")\nprint(\"Number of NaN values in the dataset BEFORE running pandas.fillna(): {:,}\".format(data.isnull().sum().sum()))\ndata = data.fillna(data.mean())\nnan_after = data.isnull().sum().sum()\nprint(\"Number of NaN values in the dataset AFTER running pandas.fillna(): {:,}\".format(nan_after))\nassert(nan_after == 0)\n\n# Separate the data into the original test and training datasets\n# Remove columns 'TARGET', 'SK_ID_CURR', 'is_train', 'is_test' as they are not features\nprint(\"\\nSeparating the training and testing dataset after completing pre-processing.\")\ntrain = data[data['is_train'] == 1]\n# Separate the 'target label' from the training dataset\ntarget = train['TARGET']\ntrain = train.drop(['TARGET', 'SK_ID_CURR', 'is_test', 'is_train'], axis=1)\ntest = data[data['is_test'] == 1]\n# To be used when preparing the submission\ntest_id = test['SK_ID_CURR']\ntest = test.drop(['TARGET', 'SK_ID_CURR', 'is_test', 'is_train'], axis=1)\nprint(\"train has {0:,} samples and {1} features.\".format(train.shape[0], train.shape[1]))\nprint(\"test has {0:,} samples and {1} features.\".format(test.shape[0], test.shape[1]))\n\n# Split 'features' and 'target label' data into training and validation data using train_test_split\n# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\nprint(\"\\nSplitting the training dataset into actual training and validation datasets\")\nX_train, X_val, y_train, y_val = train_test_split(train, target, test_size=0.2, random_state=42)\nassert(train.shape[0] == X_train.shape[0] + X_val.shape[0])\nassert(X_train.shape[1] == train.shape[1])\nassert(X_val.shape[1] == train.shape[1])\nassert(target.shape[0] == y_train.shape[0] + y_val.shape[0])\nprint(\"training dataset has {0:,} samples and {1} features.\".format(X_train.shape[0], X_train.shape[1]))\nprint(\"validating dataset has {0:,} samples and {1} features.\".format(X_val.shape[0], X_val.shape[1]))",
      "execution_count": 68,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "338d2c0b0143d6a8c74d0ff3f48a3921d5c0dfae"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "51ba916b1d62b15d1d2fc1315a5f7647903e454f"
      },
      "cell_type": "code",
      "source": "# Run GridSearchCV or fully train an estimator\nrun_mode = 'train_estimator_LGBM' # 'grid_search' or 'train_estimator_LGBM' or 'train_estimator_RFR'",
      "execution_count": 69,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d3362fd6c2b2cc65deaa4a18ecb49ed7a02d8263",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Run GridSearchCV\nif run_mode == 'grid_search':\n    perc_samples = 0.15\n    print(\"\\nPreparing to run Hyperparameters tunning with GridSearchCV using {0:.2f}% of the training samples\".format(perc_samples * 100))\n    features_train_small = X_train[:int(perc_samples * X_train.shape[0])]\n    target_train_small = y_train[:int(perc_samples * y_train.shape[0])]\n    features_val_small = X_val[:int(perc_samples * X_val.shape[0])]\n    target_val_small = y_val[:int(perc_samples * y_val.shape[0])]\n    #features_test_small = features_test[:int(perc_samples * features_test.shape[0])]\n\n    # Initialize the Estimator (Learner or Regression Model)\n    estimator = RandomForestRegressor(n_jobs=-1,\n                                      random_state=42,\n                                      verbose=0)\n\n    # Determine which Parameters to tune\n    '''\n    Tested so far:\n    parameters = {\n        'n_estimators': [9, 10, 11, 12, 13, 14, 15],\n        'criterion': ['mse', 'mae'],\n        'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7],\n        'max_features': [0.01, 0.1, 0.25, 0.45, 0.5, 0.55, 0.6, 0.75],\n        'min_samples_split': [2, 3, 4, 5],\n        'warm_start': [False, True]\n    }\n    '''\n    parameters = {\n        'n_estimators': [130, 135, 145],\n        'min_samples_leaf': [55, 62, 75],\n        'max_features': [0.2], # [0.18, 0.2, 0.23]\n        'min_samples_split': [2], # [2, 3]\n    }\n\n    # Create a scorer to measure hyperparameters performance\n    scorer = make_scorer(roc_auc_score)\n\n    # Create GridSearchCV grid object\n    grid_obj = GridSearchCV(estimator=estimator, \n                            param_grid=parameters, \n                            scoring=scorer)\n\n    # Fit the GridSearchCV grid object with the reduced training dataset and find the best hyperparameters\n    start = time()\n    grid_fit = grid_obj.fit(features_train_small, target_train_small)\n    end = time()\n    grid_fit_time = (end - start) / 60 # Ellapsed time in minutes\n    print(\"\\nGridSearchCV estimator fit time: {0:.2f} minutes\".format((end - start) / 60))\n\n    # Get the best estimator\n    best_est = grid_obj.best_estimator_\n    print(\"\\nBest Estimator: \\n{}\\n\".format(best_est))\n\n    # Get the best score\n    best_score = grid_obj.best_score_\n    print(\"\\nBest Estimator Score: {}\\n\".format(best_score))\n\n    # Get the best parameters\n    best_params = grid_obj.best_params_\n    print(\"\\nBest Hyperparameters that yield the best score: \\n{}\\n\".format(best_params))\n\n    # Make predictions with unoptimized estimator on the validation set\n    pred_val = (estimator.fit(features_train_small, target_train_small)).predict(features_val_small)\n    print(\"\\nUnoptimized Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(target_val_small, pred_val)))\n\n    # Predict with the best estimator on the validation set\n    best_pred_val = best_est.predict(features_val_small)\n    print(\"\\nOptimized Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(target_val_small, best_pred_val)))\n\n    # Predict with the best estimator on the testing set\n    #pred_test = best_est.predict(features_test)",
      "execution_count": 70,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5590b6b3ffee50b6734fb9f81a85b72b7b1e33a3"
      },
      "cell_type": "code",
      "source": "# Train estimator LGBM\nif run_mode == 'train_estimator_LGBM':\n    lgb_train = lgb.Dataset(data=X_train, label=y_train)\n    lgb_eval = lgb.Dataset(data=X_val, label=y_val)\n\n    params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n              'learning_rate': 0.01, 'num_leaves': 48, 'num_iteration': 500, 'verbose': 0 ,\n              'colsample_bytree':.8, 'subsample':.9, 'max_depth':7, 'reg_alpha':.1, 'reg_lambda':.1, \n              'min_split_gain':.01, 'min_child_weight':1}\n\n    estimator = lgb.train(params, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=20, verbose_eval=200)\n    print(\"\\nPreparing to train the following estimator: \\n{}\".format(estimator))\n\n    lgb.plot_importance(estimator, figsize=(12, 25), max_num_features=100);",
      "execution_count": 71,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "adf25ba4e24d459ac9e0fe1631a73609a6989e99",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Train estimator RandonForrestRegressor\nif run_mode == 'train_estimator_RFR':\n    # Initialize the Estimator (Learner or Regression Model) with the best hyperparameters\n    # Alternative: n_estimators=135, max_features=0.2, min_samples_split=2, min_samples_leaf=62\n    # Alternative2: criterion='mae', # default='mse', VERY SLOW\n    estimator = RandomForestRegressor(n_estimators=125, # default=10\n                                      max_features=0.2, # default='auto'\n                                      min_samples_split=2, # default=2\n                                      min_samples_leaf=75, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=0) # default=0\n    print(\"\\nPreparing to train the following estimator: \\n{}\".format(estimator))\n\n    # Fit the estimator with the training dataset\n    start = time()\n    estimator.fit(X_train, y_train)\n    end = time()\n    print(\"\\nEstimator fit time: {} seconds\".format(int(round(end - start))))\n\n    # Predict with the validation dataset\n    pred_val = estimator.predict(X_val)\n    print(\"\\nEstimator prediction score on Validation set: \\t{}\".format(roc_auc_score(y_val, pred_val)))\n    \n    # Determine the feature importance\n    fi = pd.DataFrame()\n    fi['feature'] = X_train.columns\n    fi['importance'] = estimator.feature_importances_\n    display(fi.sort_values(by=['importance'], ascending=False).head(10))\n\n    # TODO: GRAPH THE FEATURE IMPORTANCE",
      "execution_count": 72,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "85c8c3c19174276cdc1ee4adca42d90b059479ad"
      },
      "cell_type": "code",
      "source": "if 'train_estimator_' in run_mode:\n    # Predict using the 'test' dataset for submission\n    pred_test = estimator.predict(test)\n    \n    # Prepare prediction for submission\n    print(\"\\nPreparing prediction for submission.\")\n    submission = pd.DataFrame()\n    submission['SK_ID_CURR'] = test_id\n    submission['TARGET'] = pred_test\n    submission.head()\n    file_name = run_mode.split('train_estimator_')[1] + '.csv'\n    submission.to_csv(file_name, index=False)",
      "execution_count": 73,
      "outputs": [
        
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "76b5336d6fe10aa6c4464dc88796e81f58f3f8a1"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": [
        
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
