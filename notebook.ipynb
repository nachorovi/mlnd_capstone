{
  "cells": [
    {
      "metadata": {
        "_uuid": "815b9b413850530451e01206ddce063f0e7c53c3"
      },
      "cell_type": "markdown",
      "source": "Questions:\n* Is 'roc_auc_score' an appropriate metric? What else could I use? I mreged previous_application with application_train, I got a higher 'roc_auc_score' but a lower submission score.\n* When I mreged previous_application with application_train using .groupby().mean() it dropped the categorical features! How do I avoid this?\n\n* When handling categorical features. Do I need to use pd.factorize? see  https://www.kaggle.com/shivamb/homecreditrisk-extensive-eda-baseline-0-772. I'm just using get_dummies() directly.\nA: You can use both, you have to what works best for your dataset. But both can work\n* When using simple imputer, I lose the column names. It turns the pd.DataFrame into a np.ndarray. Is this the right process, or are there other ways to handle missing data, NaN, Inf, numbers too big for INT32, etc?\nA: Consider 'pd.fillna()'. https://pandas.pydata.org/pandas-docs/stable/missing_data.html\n* Should I do one-hot encoding before or after merging the different datasets (ex, application_train & previous_application)?\nA: Merger, one-hot encoding fitting the model\n* For previous_application after I run the simple imputer I don't have any NaN in the np ndarray, but when I turn it into a pandas DataFrame it has 11109336. I need to turn it into a pandas DataFrame to merge it with application_train. What can I do?\nA: Question is not clear"
    },
    {
      "metadata": {
        "_uuid": "a50ca0e6adc3396caaee0109fd82fee783d1073d"
      },
      "cell_type": "markdown",
      "source": "June 7 through June 17\nFuture steps - Must do:\n* Handle missing data or NaN, try with pd.fillna()\n* Add feature importance\n* Try other regressor: LGBM\n* Data visualization\n* Normalizing data\n* Handling skewed data\n* Using the other data files (Feature Engineering, Create flat dataset), categorical features are being dropped.\n* Try other regressors: GBRT, AdaBoost, SGDC\nReport:\n* Preliminary results\n\nJune 18 through July 1\nReport:\n* WEEK1: I should have a skeleton of all the areas of the report to share with mentor.\n* WEEK2: I should have a first full version of the report to share with mentor and ready to submit\nFuture steps - Nice to do:\n* Only apply one-hot encoding to columns which are not numeric.\n* Consider doing 'pd.factorize' for one-hot encoding\n* CREATE A scikit-learn Pipeline\n* MAKE AN ENSEMBLE METHOD OF MULTIPLE MODELS\n* Measuring model/learner performance using other evaluation metric (LR+, f1, r2)?\n* Hyperparameter tunening (different values until I find a max, use higher percentage of training & validation data)\n* Review Handling missing or invalid data, other better methods?\n* See if sklearn.model_selection.'KFold' could yield a better result than 'train_test_split'\n\nDONE:\n* Using the other data files (Feature Engineering, Create flat dataset) first pass complete\n* Handling missing or invalid data\n* Split train & validation set\n* Hyperparameter tunening\n* Measuring model/learner performance using evaluation metric (auc_roc_score)"
    },
    {
      "metadata": {
        "_uuid": "d355f28e4f031ceb02bb7223ae79f96353cff7c8"
      },
      "cell_type": "markdown",
      "source": "    Preliminary results:\n- Categorical values turned into numerical features with one-hot encoding scheme\n- Fill missing or wrong values\n- RFR Default values, only application training data, score: 0.591\n- RFR Best values with GridSearchCV, only application training data, score: 0.722 <- 0.62 (surpassing the 0.688 benchmark for Random Forrest)\n- RFR Best values with GridSearchCV, ALL available data, score: ?\n- ???? Best values with GridSearchCV, ALL available data, score: ?\n- Normalizing Data\n- Unskewing Data\n\nAbbreviations:\nRFR: Random Forest Regressor\n????: Another regressor"
    },
    {
      "metadata": {
        "_uuid": "9615588ff15ee992f488696fa0a5fa1394bb2707"
      },
      "cell_type": "markdown",
      "source": "# **Sections:**\n[1. Import libraries & support functions](#import)  \n[2. Dataset preparation](#data_prep)  \n[3. Exploratory Data Analysis (EDA)](#eda)  \n&nbsp; [3.1 Application Train](#eda_app_train)  \n[4. Feature Engineering](#feat_eng)  \n&nbsp; [4.1 Create a Flat Dataset](#flat_dataset)  \n&nbsp; [4.2 Handle Skewed Continuous Data](#4.1)  \n&nbsp; [4.3 Normalize Continuous Data](#4.2)  \n&nbsp; [4.4 Handle Categorical Variables](#4.3)  \n&nbsp; [4.5 Handle Missing Data](#4.4)  \n&nbsp;&nbsp; [4.5.1 Previous Applications](#4.5)  \n[5. Split Data into Training and Validation](#5)  \n[6. Hyperparameter Tuning](#6)  \n[7. Model Fitting & Prediction](#7)  "
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSVfile I/O (e.g. pd.read_csv)\nimport os\nfrom plotly.offline import init_notebook_mode, iplot\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom plotly import tools\n# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\nfrom sklearn.ensemble import RandomForestRegressor\n# Add evaluation metric to measure the model's performance\n# Regression metrics available:\n# http://scikit-learn.org/stable/modules/classes.html#regression-metrics\n# http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\n# http://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc\n# http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html\n# Cannot use sklearn.metrics.accuracy_score as it is a Classification metric\nfrom sklearn.metrics import make_scorer, r2_score, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom time import time\nfrom IPython.display import display # Allows the use of display() for DataFrames",
      "execution_count": 172,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "ffc696cb463655b657ded7192c08a3ced678eab1"
      },
      "cell_type": "code",
      "source": "# Enable Debugging while I test things\ndebugging = False # True or False",
      "execution_count": 173,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b0dc45408902e93d5cc4e3b7825abe8d9d5d5914"
      },
      "cell_type": "code",
      "source": "# Support functions\n'''\ndef bar_hor(df, col, title, color, w=None, h=None, lm=0, limit=100, return_trace=False, rev=False, xlb = False):\n    cnt_srs = df[col].value_counts()\n    yy = cnt_srs.head(limit).index[::-1] \n    xx = cnt_srs.head(limit).values[::-1] \n    if rev:\n        yy = cnt_srs.tail(limit).index[::-1] \n        xx = cnt_srs.tail(limit).values[::-1] \n    if xlb:\n        trace = go.Bar(y=xlb, x=xx, orientation = 'h', marker=dict(color=color))\n    else:\n        trace = go.Bar(y=yy, x=xx, orientation = 'h', marker=dict(color=color))\n    if return_trace:\n        return trace \n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n\ndef gp(col, title):\n    df1 = data_train[data_train[\"TARGET\"] == 1]\n    df0 = data_train[data_train[\"TARGET\"] == 0]\n    a1 = df1[col].value_counts()\n    b1 = df0[col].value_counts()\n    \n    total = dict(data_train[col].value_counts())\n    x0 = a1.index\n    x1 = b1.index\n    \n    y0 = [float(x)*100 / total[x0[i]] for i,x in enumerate(a1.values)]\n    y1 = [float(x)*100 / total[x1[i]] for i,x in enumerate(b1.values)]\n\n    trace1 = go.Bar(x=a1.index, y=y0, name='Target : 1', marker=dict(color=\"#96D38C\"))\n    trace2 = go.Bar(x=b1.index, y=y1, name='Target : 0', marker=dict(color=\"#FEBFB3\"))\n    return trace1, trace2 \n'''",
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 174,
          "data": {
            "text/plain": "'\\ndef bar_hor(df, col, title, color, w=None, h=None, lm=0, limit=100, return_trace=False, rev=False, xlb = False):\\n    cnt_srs = df[col].value_counts()\\n    yy = cnt_srs.head(limit).index[::-1] \\n    xx = cnt_srs.head(limit).values[::-1] \\n    if rev:\\n        yy = cnt_srs.tail(limit).index[::-1] \\n        xx = cnt_srs.tail(limit).values[::-1] \\n    if xlb:\\n        trace = go.Bar(y=xlb, x=xx, orientation = \\'h\\', marker=dict(color=color))\\n    else:\\n        trace = go.Bar(y=yy, x=xx, orientation = \\'h\\', marker=dict(color=color))\\n    if return_trace:\\n        return trace \\n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\\n    data = [trace]\\n    fig = go.Figure(data=data, layout=layout)\\n    iplot(fig)\\n\\ndef gp(col, title):\\n    df1 = data_train[data_train[\"TARGET\"] == 1]\\n    df0 = data_train[data_train[\"TARGET\"] == 0]\\n    a1 = df1[col].value_counts()\\n    b1 = df0[col].value_counts()\\n    \\n    total = dict(data_train[col].value_counts())\\n    x0 = a1.index\\n    x1 = b1.index\\n    \\n    y0 = [float(x)*100 / total[x0[i]] for i,x in enumerate(a1.values)]\\n    y1 = [float(x)*100 / total[x1[i]] for i,x in enumerate(b1.values)]\\n\\n    trace1 = go.Bar(x=a1.index, y=y0, name=\\'Target : 1\\', marker=dict(color=\"#96D38C\"))\\n    trace2 = go.Bar(x=b1.index, y=y1, name=\\'Target : 0\\', marker=dict(color=\"#FEBFB3\"))\\n    return trace1, trace2 \\n'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "_kg_hide-input": false
      },
      "cell_type": "code",
      "source": "# List available data files\n#print(os.listdir(\"../input\"))\nprint(\"Loading data files...\")\n\nstart = time()\n# Load the Point of Sale Cash balance dataset\nposc_bal = pd.read_csv(\"../input/POS_CASH_balance.csv\")\n# Load the Bureau Balance dataset\nbureau_bal = pd.read_csv(\"../input/bureau_balance.csv\")\n# Load the Application Training dataset\ndata_train = pd.read_csv(\"../input/application_train.csv\")\n# Load the Previous Applications dataset\nprev_app = pd.read_csv(\"../input/previous_application.csv\")\n# Load the Installements Payments dataset\ninst_pay = pd.read_csv(\"../input/installments_payments.csv\")\n# Load the Credit Card Balance dataset\ncc_bal = pd.read_csv(\"../input/credit_card_balance.csv\")\n# Load the Application Testing dataset\ndata_test = pd.read_csv(\"../input/application_test.csv\")\n# Load the Bureau dataset\nbureau = pd.read_csv(\"../input/bureau.csv\")\nend = time()\n\nprint(\"Finished loading data files in {0:.1f} minutes.\".format((end - start) / 60))",
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Loading data files...\nFinished loading data files in 1.1 minutes.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "10bd1b898552f438389d8bfe962181528d4aa344"
      },
      "cell_type": "code",
      "source": "'''\n# Display the first 5 records of application_train.csv\ndisplay(data_train.head())\n# Display the first 5 records of application_test.csv\ndisplay(data_test.head())\n# See the first 5 rows of the dataframe\ndisplay(prev_app.head())\n\n# DataFrame statistics summary for selected columns\ndata_train[[\"AMT_INCOME_TOTAL\", \"AMT_CREDIT\", \"AMT_ANNUITY\", \"AMT_GOODS_PRICE\"]].describe()\n#print(len(data_train.columns))\n'''",
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 176,
          "data": {
            "text/plain": "'\\n# Display the first 5 records of application_train.csv\\ndisplay(data_train.head())\\n# Display the first 5 records of application_test.csv\\ndisplay(data_test.head())\\n# See the first 5 rows of the dataframe\\ndisplay(prev_app.head())\\n\\n# DataFrame statistics summary for selected columns\\ndata_train[[\"AMT_INCOME_TOTAL\", \"AMT_CREDIT\", \"AMT_ANNUITY\", \"AMT_GOODS_PRICE\"]].describe()\\n#print(len(data_train.columns))\\n'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3fe5a388f860fb1ebdc3698221db4e0c66697726"
      },
      "cell_type": "code",
      "source": "# Total number of records\nprint(\"Total number of records in the train dataset: {:,}\".format(len(data_train)))\n# Total number of features. Excluding the load ids (SK_ID_CURR) and the target variable (TARGET).\nprint(\"Total number of features in the train dataset: {}\".format(data_train.shape[1] - 2))\n\n# Total number of records\nprint(\"Total number of records in the test dataset: {:,}\".format(len(data_test)))\n# Total number of features. Excluding the load ids (SK_ID_CURR). There is NO target variable (TARGET) in the test dataset.\nprint(\"Total number of features in the test dataset: {}\".format(data_test.shape[1] - 1))\n\n# Total number of features. Excluding the current loan ids (SK_ID_CURR) and the previous loan ids (SK_ID_PREV)\nprint(\"Total number of features of previous applications, before one-hot encoding: {}\".format(prev_app.shape[1] - 2))\nprint(\"Previous applications has {:,} samples\".format(prev_app.shape[0]))\n# Check if therer is any NaN value in the dataset\nprint(\"Total number of NaN in the dataframe: {:,}\".format(prev_app.isnull().sum().sum())) # TODO: REMOVE",
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Total number of records in the train dataset: 307,511\nTotal number of features in the train dataset: 120\nTotal number of records in the test dataset: 48,744\nTotal number of features in the test dataset: 120\nTotal number of features of previous applications, before one-hot encoding: 35\nPrevious applications has 1,670,214 samples\nTotal number of NaN in the dataframe: 11,109,336\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ee2f2a92f145a72cccd670372cd2cd6fefd0dca"
      },
      "cell_type": "code",
      "source": "# GRAPHS ARE NOT DISPLAYING\n'''\n# Target Variable Distribution \nbar_hor(data_train, \"TARGET\", \"Distribution of Target Variable\" , [\"#44ff54\", '#ff4444'], h=350, w=600, lm=200, xlb = ['Target : 1','Target : 0'])\n\ntr0 = bar_hor(data_train, \"CODE_GENDER\", \"Distribution of CODE_GENDER Variable\" ,\"#f975ae\", w=700, lm=100, return_trace= True)\ntr1, tr2 = gp('CODE_GENDER', 'Distribution of Target with Applicant Gender')\n\nfig = tools.make_subplots(rows=1, cols=3, print_grid=False, subplot_titles = [\"Gender Distribution\" , \"Gender, Target=1\" ,\"Gender, Target=0\"])\nfig.append_trace(tr0, 1, 1);\nfig.append_trace(tr1, 1, 2);\nfig.append_trace(tr2, 1, 3);\nfig['layout'].update(height=350, showlegend=False, margin=dict(l=50));\niplot(fig);\n'''",
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 178,
          "data": {
            "text/plain": "'\\n# Target Variable Distribution \\nbar_hor(data_train, \"TARGET\", \"Distribution of Target Variable\" , [\"#44ff54\", \\'#ff4444\\'], h=350, w=600, lm=200, xlb = [\\'Target : 1\\',\\'Target : 0\\'])\\n\\ntr0 = bar_hor(data_train, \"CODE_GENDER\", \"Distribution of CODE_GENDER Variable\" ,\"#f975ae\", w=700, lm=100, return_trace= True)\\ntr1, tr2 = gp(\\'CODE_GENDER\\', \\'Distribution of Target with Applicant Gender\\')\\n\\nfig = tools.make_subplots(rows=1, cols=3, print_grid=False, subplot_titles = [\"Gender Distribution\" , \"Gender, Target=1\" ,\"Gender, Target=0\"])\\nfig.append_trace(tr0, 1, 1);\\nfig.append_trace(tr1, 1, 2);\\nfig.append_trace(tr2, 1, 3);\\nfig[\\'layout\\'].update(height=350, showlegend=False, margin=dict(l=50));\\niplot(fig);\\n'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "6b45c23eb5bd269ec30f30d863cd80a7bb35d9dd"
      },
      "cell_type": "markdown",
      "source": "## <a id=\"4.1\">4.1 Feature Engineering - Create a Flat Dataset</a>"
    },
    {
      "metadata": {
        "_uuid": "cec5ec08de391840245c93255c6b94d934f4bc84"
      },
      "cell_type": "markdown",
      "source": "### <a id=\"4.1.1\">4.1.1 Feature Engineering - Create a Flat Dataset - Previous Applications</a>"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "39dcd898ed95bd5c6a44d4e0e001b61622b7a89c"
      },
      "cell_type": "code",
      "source": "'''\n# Check if therer is any NaN value in the pd dataframe df\ndf.isnull().sum().sum()\n\n# Check if therer is any NaN value in the np nparray np_array\nnp.argwhere(np.isnan(np_array))\n'''",
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 179,
          "data": {
            "text/plain": "'\\n# Check if therer is any NaN value in the pd dataframe df\\ndf.isnull().sum().sum()\\n\\n# Check if therer is any NaN value in the np nparray np_array\\nnp.argwhere(np.isnan(np_array))\\n'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "270ef25dfa972d28a962bae4981248fb1c7e3ce3"
      },
      "cell_type": "code",
      "source": "# Merge Point of Sale Cash Balance dataset\n# Count the number of previous applications for a given 'SK_ID_CURR', and create a new feature\nposc_bal_count = posc_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nposc_bal['POSC_BAL_COUNT'] = posc_bal['SK_ID_CURR'].map(posc_bal_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\nposc_bal = posc_bal.drop(['SK_ID_PREV'], axis=1)\n\n# Average values for all other features in previous applications\nposc_bal_avg = posc_bal.groupby('SK_ID_CURR').mean()\nposc_bal_avg.columns = ['pcb_' + col for col in posc_bal_avg.columns]\ndata_train = data_train.merge(right=posc_bal_avg.reset_index(), how='left', on='SK_ID_CURR')",
      "execution_count": 180,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "379a233789f277ca8630dae8c5d5232f3ff2f793"
      },
      "cell_type": "code",
      "source": "'''# Merge Bureau Balance dataset\n#'SK_ID_BUREAU'\n# Count the number of previous applications for a given 'SK_ID_CURR', and create a new feature\nbureau_bal_count = bureau_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nbureau_bal['bureau_bal_COUNT'] = bureau_bal['SK_ID_CURR'].map(bureau_bal_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\nbureau_bal = bureau_bal.drop(['SK_ID_PREV'], axis=1)\n\n# Average values for all other features in previous applications\nbureau_bal_avg = bureau_bal.groupby('SK_ID_CURR').mean()\nbureau_bal_avg.columns = ['posc_' + col for col in bureau_bal_avg.columns]\ndata_train = data_train.merge(right=bureau_bal_avg.reset_index(), how='left', on='SK_ID_CURR')\n'''",
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 181,
          "data": {
            "text/plain": "\"# Merge Bureau Balance dataset\\n#'SK_ID_BUREAU'\\n# Count the number of previous applications for a given 'SK_ID_CURR', and create a new feature\\nbureau_bal_count = bureau_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\\nbureau_bal['bureau_bal_COUNT'] = bureau_bal['SK_ID_CURR'].map(bureau_bal_count['SK_ID_PREV'])\\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\\nbureau_bal = bureau_bal.drop(['SK_ID_PREV'], axis=1)\\n\\n# Average values for all other features in previous applications\\nbureau_bal_avg = bureau_bal.groupby('SK_ID_CURR').mean()\\nbureau_bal_avg.columns = ['posc_' + col for col in bureau_bal_avg.columns]\\ndata_train = data_train.merge(right=bureau_bal_avg.reset_index(), how='left', on='SK_ID_CURR')\\n\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "1c2bce0ce0a3f4028c8857b6db48372a8a2bbb43"
      },
      "cell_type": "code",
      "source": "if debugging:\n    display(prev_app.shape)\n    display(prev_app.head())\n    display(prev_app[prev_app['SK_ID_CURR'] == 271877])\n    display(data_train.shape)\n    display(data_train.head())",
      "execution_count": 182,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "73ce63a2eadd2d575ea52b317a55c24acf09951d"
      },
      "cell_type": "code",
      "source": "# Merge Previous Applications dataset\n# Count the number of previous applications for a given 'SK_ID_CURR'\nprev_app_count = prev_app[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nprev_app['PREV_COUNT'] = prev_app['SK_ID_CURR'].map(prev_app_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\nprev_app = prev_app.drop(['SK_ID_PREV'], axis=1)\n\n# Average values for all other features in previous applications\nprev_app_avg = prev_app.groupby('SK_ID_CURR').mean()\nprev_app_avg.columns = ['pa_' + col for col in prev_app_avg.columns]\ndata_train = data_train.merge(right=prev_app_avg.reset_index(), how='left', on='SK_ID_CURR')",
      "execution_count": 183,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "e01af9767baee39a00d0fefebd69b7227b9b81c9"
      },
      "cell_type": "code",
      "source": "if debugging:\n    display(prev_app_count.head())\n    display(prev_app_avg.shape)\n    display(prev_app_avg.head())\n    #display(prev_app_avg[prev_app_avg['SK_ID_CURR'] == 271877].head())\n    display(data_train.shape)\n    display(data_train['p_AMT_ANNUITY'].head())\n    print(\"Total number of NaN in the data dataframe: {:,}\".format(data_train.isnull().sum().sum()))",
      "execution_count": 184,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "848e06dea27f5efb1847debbd5ca9c485ca61908"
      },
      "cell_type": "code",
      "source": "# Merge Installments Payments dataset\n# Count the number of installments payments for a given 'SK_ID_CURR', and create a new feature\ninst_pay_count = inst_pay[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\ninst_pay['INST_PAY_COUNT'] = inst_pay['SK_ID_CURR'].map(inst_pay_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\ninst_pay = inst_pay.drop(['SK_ID_PREV'], axis=1)\n\n## Average values for all other features in previous applications\ninst_pay_avg = inst_pay.groupby('SK_ID_CURR').mean()\ninst_pay_avg.columns = ['ip_' + col for col in inst_pay_avg.columns]\ndata_train = data_train.merge(right=inst_pay_avg.reset_index(), how='left', on='SK_ID_CURR')",
      "execution_count": 185,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "70a191e709a4ba349fea674b039fc40fa9cb1c9e"
      },
      "cell_type": "code",
      "source": "# Merge Credit Card Balance dataset\n# Count the number of previous applications for a given 'SK_ID_CURR', and create a new feature\ncc_bal_count = cc_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\ncc_bal['CC_BAL_COUNT'] = cc_bal['SK_ID_CURR'].map(cc_bal_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\ncc_bal = cc_bal.drop(['SK_ID_PREV'], axis=1)\n\n## Average values for all other features in previous applications\ncc_bal_avg = cc_bal.groupby('SK_ID_CURR').mean()\ncc_bal_avg.columns = ['ccb_' + col for col in cc_bal_avg.columns]\ndata_train = data_train.merge(right=cc_bal_avg.reset_index(), how='left', on='SK_ID_CURR')",
      "execution_count": 186,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "7130126d751a815b27cd0cf61ee835e9efb58aa3"
      },
      "cell_type": "code",
      "source": "if debugging:\n    display(bureau.head())\n    display(bureau.shape)",
      "execution_count": 187,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "a85c005254d9976eba6042afd2b06bda36cdea1a"
      },
      "cell_type": "code",
      "source": "# Merge Bureau dataset\n# Count the number of credits registered in the bureau for a given 'SK_ID_CURR', and create a new feature\nbureau_count = bureau[['SK_ID_CURR', 'SK_ID_BUREAU']].groupby('SK_ID_CURR').count()\nbureau['BUREAU_COUNT'] = bureau['SK_ID_CURR'].map(bureau_count['SK_ID_BUREAU'])\n# Remove the 'SK_ID_BUREAU' column from the dataset as it doesn't add value\nbureau = bureau.drop(['SK_ID_BUREAU'], axis=1)\n\n## Average values for all other features in previous applications\nbureau_avg = bureau.groupby('SK_ID_CURR').mean()\nbureau_avg.columns = ['b_' + col for col in bureau_avg.columns]\ndata_train = data_train.merge(right=bureau_avg.reset_index(), how='left', on='SK_ID_CURR')",
      "execution_count": 188,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "b98aece8cd7d5a096157be491f57a1361c9f6206"
      },
      "cell_type": "code",
      "source": "if debugging:\n    display(bureau_avg.head())\n    display(bureau_avg.shape)",
      "execution_count": 189,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c9e7f293121fd26492be2e8d0334f5b6551e624",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Transforming skewed continuous features",
      "execution_count": 190,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "10916d50c700ec898123f3b422bb24064f2e4711"
      },
      "cell_type": "code",
      "source": "# Normalizing numerical features\n#from sklearn.preprocessing import MinMaxScaler",
      "execution_count": 191,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "95b3623001c0e8d93e84e3a9e6811264b7db861f"
      },
      "cell_type": "code",
      "source": "# Turn categorical variables into numerical features using the one-hot encoding scheme\n\n# Total number of features. Excluding the load ids (SK_ID_CURR) and the target variable (TARGET)\nprint(\"Total number of features of train data, before one-hot encoding: {}\".format(data_train.shape[1] - 2))\n# Total number of features. Excluding the load ids (SK_ID_CURR). The test dataset has NO target label (TARGET)\nprint(\"Total number of features of test data, before one-hot encoding: {}\".format(data_test.shape[1] - 1))\n\n# One-hot encoding\ndata_train_encoded = pd.get_dummies(data_train)\ndata_test_encoded = pd.get_dummies(data_test)\n\n# Total number of features. Excluding the loan ids (SK_ID_CURR) and the target variable (TARGET)\nprint(\"Total number of features of train data, after one-hot encoding: {}\".format(data_train_encoded.shape[1] - 2))\n# Total number of features. Excluding the loan ids (SK_ID_CURR). The test dataset has NO target label (TARGET)\nprint(\"Total number of features of test data, after one-hot encoding: {}\".format(data_test_encoded.shape[1] - 1))\n\n# New list of features\n#print(list(data_train_encoded.columns))\n\n# Determine what columns are missing\ntrain_list = list(data_train_encoded.columns)\ntest_list = list(data_test_encoded.columns)\ndifference = [e for e in train_list if e not in test_list]\nprint(difference)\n\n# Add those columns to the test set will all zeros\ndata_test_encoded_complete = data_test_encoded\nfor e in difference:\n    if e != 'TARGET':\n        data_test_encoded_complete[e] = 0\n\nprint(\"Total number of features of test data, after one-hot encoding: {}\".format(data_test_encoded_complete.shape[1] - 1))",
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Total number of features of train data, before one-hot encoding: 187\nTotal number of features of test data, before one-hot encoding: 120\nTotal number of features of train data, after one-hot encoding: 311\nTotal number of features of test data, after one-hot encoding: 241\n['TARGET', 'pcb_MONTHS_BALANCE', 'pcb_CNT_INSTALMENT', 'pcb_CNT_INSTALMENT_FUTURE', 'pcb_SK_DPD', 'pcb_SK_DPD_DEF', 'pcb_POSC_BAL_COUNT', 'pa_AMT_ANNUITY', 'pa_AMT_APPLICATION', 'pa_AMT_CREDIT', 'pa_AMT_DOWN_PAYMENT', 'pa_AMT_GOODS_PRICE', 'pa_HOUR_APPR_PROCESS_START', 'pa_NFLAG_LAST_APPL_IN_DAY', 'pa_RATE_DOWN_PAYMENT', 'pa_RATE_INTEREST_PRIMARY', 'pa_RATE_INTEREST_PRIVILEGED', 'pa_DAYS_DECISION', 'pa_SELLERPLACE_AREA', 'pa_CNT_PAYMENT', 'pa_DAYS_FIRST_DRAWING', 'pa_DAYS_FIRST_DUE', 'pa_DAYS_LAST_DUE_1ST_VERSION', 'pa_DAYS_LAST_DUE', 'pa_DAYS_TERMINATION', 'pa_NFLAG_INSURED_ON_APPROVAL', 'pa_PREV_COUNT', 'ip_NUM_INSTALMENT_VERSION', 'ip_NUM_INSTALMENT_NUMBER', 'ip_DAYS_INSTALMENT', 'ip_DAYS_ENTRY_PAYMENT', 'ip_AMT_INSTALMENT', 'ip_AMT_PAYMENT', 'ip_INST_PAY_COUNT', 'ccb_MONTHS_BALANCE', 'ccb_AMT_BALANCE', 'ccb_AMT_CREDIT_LIMIT_ACTUAL', 'ccb_AMT_DRAWINGS_ATM_CURRENT', 'ccb_AMT_DRAWINGS_CURRENT', 'ccb_AMT_DRAWINGS_OTHER_CURRENT', 'ccb_AMT_DRAWINGS_POS_CURRENT', 'ccb_AMT_INST_MIN_REGULARITY', 'ccb_AMT_PAYMENT_CURRENT', 'ccb_AMT_PAYMENT_TOTAL_CURRENT', 'ccb_AMT_RECEIVABLE_PRINCIPAL', 'ccb_AMT_RECIVABLE', 'ccb_AMT_TOTAL_RECEIVABLE', 'ccb_CNT_DRAWINGS_ATM_CURRENT', 'ccb_CNT_DRAWINGS_CURRENT', 'ccb_CNT_DRAWINGS_OTHER_CURRENT', 'ccb_CNT_DRAWINGS_POS_CURRENT', 'ccb_CNT_INSTALMENT_MATURE_CUM', 'ccb_SK_DPD', 'ccb_SK_DPD_DEF', 'ccb_CC_BAL_COUNT', 'b_DAYS_CREDIT', 'b_CREDIT_DAY_OVERDUE', 'b_DAYS_CREDIT_ENDDATE', 'b_DAYS_ENDDATE_FACT', 'b_AMT_CREDIT_MAX_OVERDUE', 'b_CNT_CREDIT_PROLONG', 'b_AMT_CREDIT_SUM', 'b_AMT_CREDIT_SUM_DEBT', 'b_AMT_CREDIT_SUM_LIMIT', 'b_AMT_CREDIT_SUM_OVERDUE', 'b_DAYS_CREDIT_UPDATE', 'b_AMT_ANNUITY', 'b_BUREAU_COUNT', 'CODE_GENDER_XNA', 'NAME_INCOME_TYPE_Maternity leave', 'NAME_FAMILY_STATUS_Unknown']\nTotal number of features of test data, after one-hot encoding: 311\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c29b942e0d37f6e2fc3249da9c88c9bbb2b0a84"
      },
      "cell_type": "code",
      "source": "# Separate the target label from the train dataset. The column we are interested is 'TARGET'. Name it target_train.\ntarget_train = data_train['TARGET']\nprint(\"Training target label has {:,} samples\".format(target_train.shape[0]))\n\n# Remove target label from the train dataset and rename to features_train.\nfeatures_train = data_train_encoded.drop(['TARGET'], axis=1)\n\n# Test data has no taget label 'TARGET' in the dataset\nfeatures_test = data_test_encoded_complete",
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Training target label has 307,511 samples\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9f0fe582933f1a9086f982e0629a17ae1f9a266"
      },
      "cell_type": "code",
      "source": "# Handle missing values, alternative\n# https://pandas.pydata.org/pandas-docs/stable/missing_data.html\n# Previously used:\n# https://www.kaggle.com/dansbecker/handling-missing-values\n# http://scikit-learn.org/dev/modules/generated/sklearn.impute.SimpleImputer.html\n# But the result of that operation was a numpy.ndarray instead of a pandas.DataFrame\n\nprint(\"Total number of NaN in the training dataset before applying '.fillna(): {:,}\".format(features_train.isnull().sum().sum()))\nprint(\"Total number of NaN in the testing dataset before applying '.fillna(): {:,}\".format(features_train.isnull().sum().sum()))\n# Fill missing values with the column mean\nfeatures_train = features_train.fillna(features_train.mean())\nfeatures_test = features_test.fillna(features_test.mean())\nprint(\"Total number of NaN in the training dataset after applying '.fillna()': {:,}\".format(features_train.isnull().sum().sum()))\nprint(\"Total number of NaN in the testing dataset after applying '.fillna()': {:,}\".format(features_train.isnull().sum().sum()))",
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Total number of NaN in the dataframe before applying '.fillna(): 15,266,408\nTotal number of NaN in the dataframe after applying '.fillna()': 0\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 200,
          "data": {
            "text/plain": "'\\n# Filling the missing \\ndisplay(features_train.head())\\ndisplay(features_train.isnull().sum().sum())\\nnew_features_train = features_train.fillna(features_train.mean())\\n'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1de08efe6afcbc5ce61cc3711f9c250d07ec96c6"
      },
      "cell_type": "code",
      "source": "# Shuffle and split the data\n# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\nfrom sklearn.model_selection import train_test_split\n\n# Split the 'features' and 'target label' data into training and validating sets\nX_train, X_val, y_train, y_val = train_test_split(features_train,\n                                                  target_train,\n                                                  test_size=0.2,\n                                                  random_state=42)\n\nprint(\"Original Training set has {:,} samples.\".format(features_train.shape[0]))\nprint(\"After split Training set has {:,} samples.\".format(X_train.shape[0]))\nprint(\"After split Validating set has {:,} samples.\".format(X_val.shape[0]))\nprint(\"Testing set has {:,} samples.\".format(data_test.shape[0]))",
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Original Training set has 307,511 samples.\nAfter split Training set has 246,008 samples.\nAfter split Validating set has 61,503 samples.\nTesting set has 48,744 samples.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "51ba916b1d62b15d1d2fc1315a5f7647903e454f"
      },
      "cell_type": "code",
      "source": "# Run GridSearchCV or fully train an estimator\nrun_mode = 'train_estimator' # 'grid_search' or 'train_estimator'",
      "execution_count": 156,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d3362fd6c2b2cc65deaa4a18ecb49ed7a02d8263"
      },
      "cell_type": "code",
      "source": "# Run GridSearchCV\nif run_mode == 'grid_search':\n    perc_samples = 0.15\n    print(\"Preparing to run Hyperparameters tunning with GridSearchCV using {0:.2f}% of the training samples\".format(perc_samples * 100))\n    features_train_small = X_train[:int(perc_samples * X_train.shape[0])]\n    target_train_small = y_train[:int(perc_samples * y_train.shape[0])]\n    features_val_small = X_val[:int(perc_samples * X_val.shape[0])]\n    target_val_small = y_val[:int(perc_samples * y_val.shape[0])]\n    #features_test_small = features_test[:int(perc_samples * features_test.shape[0])]\n\n    # Initialize the Estimator (Learner or Regression Model)\n    estimator = RandomForestRegressor(n_jobs=-1,\n                                      random_state=42,\n                                      verbose=0)\n\n    # Determine which Parameters to tune\n    '''\n    Tested so far:\n    parameters = {\n        'n_estimators': [9, 10, 11, 12, 13, 14, 15],\n        'criterion': ['mse', 'mae'],\n        'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7],\n        'max_features': [0.01, 0.1, 0.25, 0.45, 0.5, 0.55, 0.6, 0.75],\n        'min_samples_split': [2, 3, 4, 5],\n        'warm_start': [False, True]\n    }\n    '''\n    parameters = {\n        'n_estimators': [130, 135, 145],\n        'min_samples_leaf': [55, 62, 75],\n        'max_features': [0.2], # [0.18, 0.2, 0.23]\n        'min_samples_split': [2], # [2, 3]\n    }\n\n    # Create a scorer to measure hyperparameters performance\n    scorer = make_scorer(roc_auc_score)\n\n    # Create GridSearchCV grid object\n    grid_obj = GridSearchCV(estimator=estimator, \n                            param_grid=parameters, \n                            scoring=scorer)\n\n    # Fit the GridSearchCV grid object with the reduced training dataset and find the best hyperparameters\n    start = time()\n    grid_fit = grid_obj.fit(features_train_small, target_train_small)\n    end = time()\n    grid_fit_time = (end - start) / 60 # Ellapsed time in minutes\n    print(\"GridSearchCV estimator fit time: {0:.2f} minutes\".format((end - start) / 60))\n\n    # Get the best estimator\n    best_est = grid_obj.best_estimator_\n    print(\"Best Estimator: \\n{}\\n\".format(best_est))\n\n    # Get the best score\n    best_score = grid_obj.best_score_\n    print(\"Best Estimator Score: {}\\n\".format(best_score))\n\n    # Get the best parameters\n    best_params = grid_obj.best_params_\n    print(\"Best Hyperparameters that yield the best score: \\n{}\\n\".format(best_params))\n\n    # Make predictions with unoptimized estimator on the validation set\n    pred_val = (estimator.fit(features_train_small, target_train_small)).predict(features_val_small)\n    print(\"Unoptimized Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(target_val_small, pred_val)))\n\n    # Predict with the best estimator on the validation set\n    best_pred_val = best_est.predict(features_val_small)\n    print(\"Optimized Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(target_val_small, best_pred_val)))\n\n    # Predict with the best estimator on the testing set\n    #pred_test = best_est.predict(features_test)",
      "execution_count": 157,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06daaf465dc5e34eab324a1ecd7b3c80183b73ed"
      },
      "cell_type": "code",
      "source": "# Train estimator\n# TODO: rename or remove '_small', it might be '_full' or nothing\nif run_mode == 'train_estimator':\n    # Use the full training and validation datasets to fit the estimator with the best hyperparameters\n    perc_samples = 1\n    print(\"Preparing to train an estimator using {0:.2f}% of the training dataset\".format(perc_samples * 100))\n    features_train_small = X_train[:int(perc_samples * X_train.shape[0])]\n    target_train_small = y_train[:int(perc_samples * y_train.shape[0])]\n    features_val_small = X_val[:int(perc_samples * X_val.shape[0])]\n    target_val_small = y_val[:int(perc_samples * y_val.shape[0])]\n    features_test_small = features_test[:int(perc_samples * features_test.shape[0])]\n\n    # Initialize the Estimator (Learner or Regression Model) with the best hyperparameters\n    '''\n    estimator = RandomForestRegressor(criterion='mae', # default='mse', VERY SLOW\n                                      min_samples_split=2, # default=2\n                                      warm_start=False) # default=False\n    Best Performance:\n    estimator = RandomForestRegressor(n_estimators=135, # default=10\n                                      max_features=0.2, # default='auto'\n                                      min_samples_split=2, # default=2\n                                      min_samples_leaf=62, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=2) # default=0\n                                      ROC_AUC_SCORE:  0.7289233660986658 (% samples: 0.1, Data: ALL)\n                                      LEADERBOARD SCORE: \n\n    estimator = RandomForestRegressor(n_estimators=125, # default=10\n                                      max_features=0.2, # default='auto'\n                                      min_samples_split=2, # default=2\n                                      min_samples_leaf=75, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=2) # default=0\n                                      ROC_AUC_SCORE:  0.7417496532130599 (% samples: 0.15)\n                                      LEADERBOARD SCORE: 0.722 (NO improvement)\n\n    estimator = RandomForestRegressor(n_estimators=100, # default=10\n                                      max_features=0.2, # default='auto'\n                                      min_samples_split=3, # default=2\n                                      min_samples_leaf=50, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=2) # default=0\n                                      ROC_AUC_SCORE:  0.7286786065442892 (% samples: 0.1)\n                                      LEADERBOARD SCORE: 0.722\n\n    estimator = RandomForestRegressor(n_estimators=12, # default=10\n                                      max_features=0.45, # default='auto'\n                                      min_samples_leaf=2, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=2) # default=0\n                                      LEADERBOARD SCORE: 0.62\n    '''\n    estimator = RandomForestRegressor(n_estimators=135, # default=10\n                                      max_features=0.2, # default='auto'\n                                      min_samples_split=2, # default=2\n                                      min_samples_leaf=62, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=0) # default=0\n\n    # Fit the estimator with the training dataset\n    start = time()\n    estimator.fit(features_train_small, target_train_small)\n    end = time()\n    print(\"Estimator fit time: {0:.2f} minutes\".format((end - start) / 60))\n\n    # Predict with the validation dataset\n    pred_val = estimator.predict(features_val_small)\n    print(\"Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(target_val_small, pred_val)))\n    \n    # Predict using the 'test' dataset for submission\n    pred_test = estimator.predict(features_test_small)\n    #pred_test = estimator.predict(features_test)\n\n    # Prepare prediction for submission\n    submission = pd.DataFrame()\n    # Need to replace data_test_encoded_complete\n    submission['SK_ID_CURR'] = data_test_encoded_complete['SK_ID_CURR'][:int(perc_samples * features_test.shape[0])]\n    submission['TARGET'] = pred_test\n    submission.head()\n    submission.to_csv('RFR.csv', index=False)",
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Preparing to train an estimator using 100.00% of the training dataset\nEstimator fit time: 6.43 minutes\nEstimator prediction score on Validation set: \t0.7591728947854315\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f472aca96bff8a819a9fd3fa2ad7f063da3b832"
      },
      "cell_type": "code",
      "source": "# PROBLEM: features_train_small is a np.ndarray and not a pd.DataFrame. I need to fix that first.\n#fi = pd.DataFrame()\n#fi['feature'] = features_train_small.columns\n#fi['importante'] = estimator.feature_importances_",
      "execution_count": 171,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "21b229e56809d4a051efbc2e60881a2db0bca0fc"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "137160a086d9b131f241122163461ef553503943"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}