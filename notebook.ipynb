{
  "cells": [
    {
      "metadata": {
        "_uuid": "815b9b413850530451e01206ddce063f0e7c53c3"
      },
      "cell_type": "markdown",
      "source": "Questions:\n* Is 'roc_auc_score' an appropriate metric? What else could I use? I mreged previous_application with application_train, I got a higher 'roc_auc_score' but a lower submission score.\n* When I mreged previous_application with application_train using .groupby().mean() it dropped the categorical features! How do I avoid this?\n\n* When handling categorical features. Do I need to use pd.factorize? see  https://www.kaggle.com/shivamb/homecreditrisk-extensive-eda-baseline-0-772. I'm just using get_dummies() directly.\nA: You can use both, you have to what works best for your dataset. But both can work\n* When using simple imputer, I lose the column names. It turns the pd.DataFrame into a np.ndarray. Is this the right process, or are there other ways to handle missing data, NaN, Inf, numbers too big for INT32, etc?\nA: Consider 'pd.fillna()'. https://pandas.pydata.org/pandas-docs/stable/missing_data.html\n* Should I do one-hot encoding before or after merging the different datasets (ex, application_train & previous_application)?\nA: Merger, one-hot encoding fitting the model\n* For previous_application after I run the simple imputer I don't have any NaN in the np ndarray, but when I turn it into a pandas DataFrame it has 11109336. I need to turn it into a pandas DataFrame to merge it with application_train. What can I do?\nA: Question is not clear"
    },
    {
      "metadata": {
        "_uuid": "a50ca0e6adc3396caaee0109fd82fee783d1073d"
      },
      "cell_type": "markdown",
      "source": "June 7 through June 17\nFuture steps - Must do:\n* Remove SK_ID_CURR from dataset before fitting the model\n* Handle missing and categorical variables for the Flat (ALL) dataset before splitting the data. Explain why.\n* Data visualization\n* Add feature importance graph for RFR\n* Normalizing data\n* Handling skewed data\n* Using the other data files (Feature Engineering, Create flat dataset), categorical features are being dropped.\n* Try other regressors: GBRT, AdaBoost, SGDC\n* Try other regressor: LGBM - See why performance is low\nReport:\n* Preliminary results\n\nJune 18 through July 1\nReport:\n* WEEK1: I should have a skeleton of all the areas of the report to share with mentor.\n* WEEK2: I should have a first full version of the report to share with mentor and ready to submit\nFuture steps - Nice to do:\n* Only apply one-hot encoding to columns which are not numeric.\n* Consider doing 'pd.factorize' for one-hot encoding\n* CREATE A scikit-learn Pipeline\n* MAKE AN ENSEMBLE METHOD OF MULTIPLE MODELS\n* Measuring model/learner performance using other evaluation metric (LR+, f1, r2)?\n* Hyperparameter tunening (different values until I find a max, use higher percentage of training & validation data)\n* Review Handling missing or invalid data, other better methods?\n* See if sklearn.model_selection.'KFold' could yield a better result than 'train_test_split'\n\nDONE:\n* Add feature importance - For LGBM & RFR (without graph)\n* Try other regressor: LGBM - First pass complete, performance is much lower than example\n* Using the other data files (Feature Engineering, Create flat dataset) first pass complete\n* Handling missing or invalid data\n* Split train & validation set\n* Hyperparameter tunening\n* Measuring model/learner performance using evaluation metric (auc_roc_score)\n* Handle missing data or NaN, try with pd.fillna()"
    },
    {
      "metadata": {
        "_uuid": "d355f28e4f031ceb02bb7223ae79f96353cff7c8"
      },
      "cell_type": "markdown",
      "source": "    Preliminary results:\n- Categorical values turned into numerical features with one-hot encoding scheme\n- Fill missing or wrong values\n- RFR Default values, only application training data, score: 0.591\n- RFR Best values with GridSearchCV, only application training data, score: 0.722 <- 0.62 (surpassing the 0.688 benchmark for Random Forrest)\n- RFR Best values with GridSearchCV, ALL available data, score: ?\n- ???? Best values with GridSearchCV, ALL available data, score: ?\n- Handling missing data (Why don't they do it?)\n- Handling Categorical variables, why do they use pd.factorize?\n- Explain why I keep dataset merged before separating train and validation set. The mean used is more relevant, given that the test data set with be fewer samples. The reason to do one-hot encoding all together is because both the training and validation dataset need to have the same features, and doing it togethers ensures this.\n- Normalizing Data (PENDING)\n- Unskewing Data (PENDING)\n\nAbbreviations:\nRFR: Random Forest Regressor\n????: Another regressor"
    },
    {
      "metadata": {
        "_uuid": "9615588ff15ee992f488696fa0a5fa1394bb2707"
      },
      "cell_type": "markdown",
      "source": "# **Sections:**\n[1. Import libraries & support functions](#import)  \n[2. Dataset preparation](#data_prep)  \n[3. Exploratory Data Analysis (EDA)](#eda)  \n&nbsp; [3.1 Application Train](#eda_app_train)  \n[4. Feature Engineering](#feat_eng)  \n&nbsp; [4.1 Create a Flat Dataset](#flat_dataset)  \n&nbsp; [4.2 Handle Skewed Continuous Data](#4.1)  \n&nbsp; [4.3 Normalize Continuous Data](#4.2)  \n&nbsp; [4.4 Handle Categorical Variables](#4.3)  \n&nbsp; [4.5 Handle Missing Data](#4.4)  \n&nbsp;&nbsp; [4.5.1 Previous Applications](#4.5)  \n[5. Split Data into Training and Validation](#5)  \n[6. Hyperparameter Tuning](#6)  \n[7. Model Fitting & Prediction](#7)  "
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSVfile I/O (e.g. pd.read_csv)\nimport os\nfrom plotly.offline import init_notebook_mode, iplot\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom plotly import tools\n# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\nfrom sklearn.ensemble import RandomForestRegressor\n# Add evaluation metric to measure the model's performance\n# Regression metrics available:\n# http://scikit-learn.org/stable/modules/classes.html#regression-metrics\n# http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\n# http://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc\n# http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html\n# Cannot use sklearn.metrics.accuracy_score as it is a Classification metric\nfrom sklearn.metrics import make_scorer, r2_score, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom time import time\nfrom IPython.display import display # Allows the use of display() for DataFrames",
      "execution_count": 81,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "ffc696cb463655b657ded7192c08a3ced678eab1"
      },
      "cell_type": "code",
      "source": "# Enable Debugging while I test things\ndebugging = False # True or False",
      "execution_count": 82,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b0dc45408902e93d5cc4e3b7825abe8d9d5d5914",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Support functions\n'''\ndef bar_hor(df, col, title, color, w=None, h=None, lm=0, limit=100, return_trace=False, rev=False, xlb = False):\n    cnt_srs = df[col].value_counts()\n    yy = cnt_srs.head(limit).index[::-1] \n    xx = cnt_srs.head(limit).values[::-1] \n    if rev:\n        yy = cnt_srs.tail(limit).index[::-1] \n        xx = cnt_srs.tail(limit).values[::-1] \n    if xlb:\n        trace = go.Bar(y=xlb, x=xx, orientation = 'h', marker=dict(color=color))\n    else:\n        trace = go.Bar(y=yy, x=xx, orientation = 'h', marker=dict(color=color))\n    if return_trace:\n        return trace \n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n\ndef gp(col, title):\n    df1 = data_train[data_train[\"TARGET\"] == 1]\n    df0 = data_train[data_train[\"TARGET\"] == 0]\n    a1 = df1[col].value_counts()\n    b1 = df0[col].value_counts()\n    \n    total = dict(data_train[col].value_counts())\n    x0 = a1.index\n    x1 = b1.index\n    \n    y0 = [float(x)*100 / total[x0[i]] for i,x in enumerate(a1.values)]\n    y1 = [float(x)*100 / total[x1[i]] for i,x in enumerate(b1.values)]\n\n    trace1 = go.Bar(x=a1.index, y=y0, name='Target : 1', marker=dict(color=\"#96D38C\"))\n    trace2 = go.Bar(x=b1.index, y=y1, name='Target : 0', marker=dict(color=\"#FEBFB3\"))\n    return trace1, trace2 \n'''",
      "execution_count": 83,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "_kg_hide-input": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# List available data files\n#print(os.listdir(\"../input\"))\nprint(\"Loading data files...\")\n\nstart = time()\n# Load the Point of Sale Cash balance dataset\nposc_bal = pd.read_csv(\"../input/POS_CASH_balance.csv\")\n# Load the Bureau Balance dataset\nbureau_bal = pd.read_csv(\"../input/bureau_balance.csv\")\n# Load the Application Training dataset\ndata_train = pd.read_csv(\"../input/application_train.csv\")\n# Load the Previous Applications dataset\nprev_app = pd.read_csv(\"../input/previous_application.csv\")\n# Load the Installements Payments dataset\ninst_pay = pd.read_csv(\"../input/installments_payments.csv\")\n# Load the Credit Card Balance dataset\ncc_bal = pd.read_csv(\"../input/credit_card_balance.csv\")\n# Load the Application Testing dataset\ndata_test = pd.read_csv(\"../input/application_test.csv\")\n# Load the Bureau dataset\nbureau = pd.read_csv(\"../input/bureau.csv\")\nend = time()\n\nprint(\"Finished loading data files in {} seconds.\".format(end - start))",
      "execution_count": 84,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "10bd1b898552f438389d8bfe962181528d4aa344",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "'''\n# Display the first 5 records of application_train.csv\ndisplay(data_train.head())\n# Display the first 5 records of application_test.csv\ndisplay(data_test.head())\n# See the first 5 rows of the dataframe\ndisplay(prev_app.head())\n\n# DataFrame statistics summary for selected columns\ndata_train[[\"AMT_INCOME_TOTAL\", \"AMT_CREDIT\", \"AMT_ANNUITY\", \"AMT_GOODS_PRICE\"]].describe()\n#print(len(data_train.columns))\n'''",
      "execution_count": 85,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3fe5a388f860fb1ebdc3698221db4e0c66697726",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Total number of records\nprint(\"Total number of records in the train dataset: {:,}\".format(len(data_train)))\n# Total number of features. Excluding the load ids (SK_ID_CURR) and the target variable (TARGET).\nprint(\"Total number of features in the train dataset: {}\".format(data_train.shape[1] - 2))\n\n# Total number of records\nprint(\"Total number of records in the test dataset: {:,}\".format(len(data_test)))\n# Total number of features. Excluding the load ids (SK_ID_CURR). There is NO target variable (TARGET) in the test dataset.\nprint(\"Total number of features in the test dataset: {}\".format(data_test.shape[1] - 1))\n\n# Total number of features. Excluding the current loan ids (SK_ID_CURR) and the previous loan ids (SK_ID_PREV)\nprint(\"Total number of features of previous applications, before one-hot encoding: {}\".format(prev_app.shape[1] - 2))\nprint(\"Previous applications has {:,} samples\".format(prev_app.shape[0]))\n# Check if therer is any NaN value in the dataset\nprint(\"Total number of NaN in the dataframe: {:,}\".format(prev_app.isnull().sum().sum())) # TODO: REMOVE",
      "execution_count": 86,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ee2f2a92f145a72cccd670372cd2cd6fefd0dca",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# GRAPHS ARE NOT DISPLAYING\n'''\n# Target Variable Distribution \nbar_hor(data_train, \"TARGET\", \"Distribution of Target Variable\" , [\"#44ff54\", '#ff4444'], h=350, w=600, lm=200, xlb = ['Target : 1','Target : 0'])\n\ntr0 = bar_hor(data_train, \"CODE_GENDER\", \"Distribution of CODE_GENDER Variable\" ,\"#f975ae\", w=700, lm=100, return_trace= True)\ntr1, tr2 = gp('CODE_GENDER', 'Distribution of Target with Applicant Gender')\n\nfig = tools.make_subplots(rows=1, cols=3, print_grid=False, subplot_titles = [\"Gender Distribution\" , \"Gender, Target=1\" ,\"Gender, Target=0\"])\nfig.append_trace(tr0, 1, 1);\nfig.append_trace(tr1, 1, 2);\nfig.append_trace(tr2, 1, 3);\nfig['layout'].update(height=350, showlegend=False, margin=dict(l=50));\niplot(fig);\n'''",
      "execution_count": 87,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6b45c23eb5bd269ec30f30d863cd80a7bb35d9dd"
      },
      "cell_type": "markdown",
      "source": "## <a id=\"4.1\">4.1 Feature Engineering - Create a Flat Dataset</a>"
    },
    {
      "metadata": {
        "_uuid": "cec5ec08de391840245c93255c6b94d934f4bc84"
      },
      "cell_type": "markdown",
      "source": "### <a id=\"4.1.1\">4.1.1 Feature Engineering - Create a Flat Dataset - Previous Applications</a>"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "1f1b7d135accee867bddc35318ddffdd15b622ee"
      },
      "cell_type": "code",
      "source": "# Keep a copy of the application_train dataset without merging with the rest of the datasets\napp_train = data_train.copy()\napp_test = data_test.copy()",
      "execution_count": 88,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "39dcd898ed95bd5c6a44d4e0e001b61622b7a89c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "'''\n# Check if therer is any NaN value in the pd dataframe df\ndf.isnull().sum().sum()\n\n# Check if therer is any NaN value in the np nparray np_array\nnp.argwhere(np.isnan(np_array))\n'''",
      "execution_count": 89,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "270ef25dfa972d28a962bae4981248fb1c7e3ce3",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Merge Point of Sale Cash Balance dataset\n# Count the number of previous applications for a given 'SK_ID_CURR', and create a new feature\nposc_bal_count = posc_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nposc_bal['POSC_BAL_COUNT'] = posc_bal['SK_ID_CURR'].map(posc_bal_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\nposc_bal = posc_bal.drop(['SK_ID_PREV'], axis=1)\n\n# Average values for all other features in previous applications\nposc_bal_avg = posc_bal.groupby('SK_ID_CURR').mean()\nposc_bal_avg.columns = ['pcb_' + col for col in posc_bal_avg.columns]\ndata_train = data_train.merge(right=posc_bal_avg.reset_index(), how='left', on='SK_ID_CURR')",
      "execution_count": 90,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "379a233789f277ca8630dae8c5d5232f3ff2f793",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "'''# Merge Bureau Balance dataset\n#'SK_ID_BUREAU'\n# Count the number of previous applications for a given 'SK_ID_CURR', and create a new feature\nbureau_bal_count = bureau_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nbureau_bal['bureau_bal_COUNT'] = bureau_bal['SK_ID_CURR'].map(bureau_bal_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\nbureau_bal = bureau_bal.drop(['SK_ID_PREV'], axis=1)\n\n# Average values for all other features in previous applications\nbureau_bal_avg = bureau_bal.groupby('SK_ID_CURR').mean()\nbureau_bal_avg.columns = ['posc_' + col for col in bureau_bal_avg.columns]\ndata_train = data_train.merge(right=bureau_bal_avg.reset_index(), how='left', on='SK_ID_CURR')\n'''",
      "execution_count": 91,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "0936aa22007519ec1df7bb385e4f1a8f6a2b67ff"
      },
      "cell_type": "code",
      "source": "if debugging:\n    display(prev_app.shape)\n    display(prev_app.head())\n    display(prev_app[prev_app['SK_ID_CURR'] == 271877])\n    display(data_train.shape)\n    display(data_train.head())",
      "execution_count": 92,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "73ce63a2eadd2d575ea52b317a55c24acf09951d"
      },
      "cell_type": "code",
      "source": "# Merge Previous Applications dataset\n# Count the number of previous applications for a given 'SK_ID_CURR'\nprev_app_count = prev_app[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nprev_app['PREV_COUNT'] = prev_app['SK_ID_CURR'].map(prev_app_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\nprev_app = prev_app.drop(['SK_ID_PREV'], axis=1)\n\n# Average values for all other features in previous applications\nprev_app_avg = prev_app.groupby('SK_ID_CURR').mean()\nprev_app_avg.columns = ['pa_' + col for col in prev_app_avg.columns]\ndata_train = data_train.merge(right=prev_app_avg.reset_index(), how='left', on='SK_ID_CURR')",
      "execution_count": 93,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "41116d2861dc78d2b9f059563f6fb4098b11cc05"
      },
      "cell_type": "code",
      "source": "if debugging:\n    display(prev_app_count.head())\n    display(prev_app_avg.shape)\n    display(prev_app_avg.head())\n    #display(prev_app_avg[prev_app_avg['SK_ID_CURR'] == 271877].head())\n    display(data_train.shape)\n    display(data_train['p_AMT_ANNUITY'].head())\n    print(\"Total number of NaN in the data dataframe: {:,}\".format(data_train.isnull().sum().sum()))",
      "execution_count": 94,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "848e06dea27f5efb1847debbd5ca9c485ca61908"
      },
      "cell_type": "code",
      "source": "# Merge Installments Payments dataset\n# Count the number of installments payments for a given 'SK_ID_CURR', and create a new feature\ninst_pay_count = inst_pay[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\ninst_pay['INST_PAY_COUNT'] = inst_pay['SK_ID_CURR'].map(inst_pay_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\ninst_pay = inst_pay.drop(['SK_ID_PREV'], axis=1)\n\n## Average values for all other features in previous applications\ninst_pay_avg = inst_pay.groupby('SK_ID_CURR').mean()\ninst_pay_avg.columns = ['ip_' + col for col in inst_pay_avg.columns]\ndata_train = data_train.merge(right=inst_pay_avg.reset_index(), how='left', on='SK_ID_CURR')",
      "execution_count": 95,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "70a191e709a4ba349fea674b039fc40fa9cb1c9e"
      },
      "cell_type": "code",
      "source": "# Merge Credit Card Balance dataset\n# Count the number of previous applications for a given 'SK_ID_CURR', and create a new feature\ncc_bal_count = cc_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\ncc_bal['CC_BAL_COUNT'] = cc_bal['SK_ID_CURR'].map(cc_bal_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\ncc_bal = cc_bal.drop(['SK_ID_PREV'], axis=1)\n\n## Average values for all other features in previous applications\ncc_bal_avg = cc_bal.groupby('SK_ID_CURR').mean()\ncc_bal_avg.columns = ['ccb_' + col for col in cc_bal_avg.columns]\ndata_train = data_train.merge(right=cc_bal_avg.reset_index(), how='left', on='SK_ID_CURR')",
      "execution_count": 96,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "cd03b77d4a01d0d8a7cbb114b1620c83144c628c"
      },
      "cell_type": "code",
      "source": "if debugging:\n    display(bureau.head())\n    display(bureau.shape)",
      "execution_count": 97,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "a85c005254d9976eba6042afd2b06bda36cdea1a"
      },
      "cell_type": "code",
      "source": "# Merge Bureau dataset\n# Count the number of credits registered in the bureau for a given 'SK_ID_CURR', and create a new feature\nbureau_count = bureau[['SK_ID_CURR', 'SK_ID_BUREAU']].groupby('SK_ID_CURR').count()\nbureau['BUREAU_COUNT'] = bureau['SK_ID_CURR'].map(bureau_count['SK_ID_BUREAU'])\n# Remove the 'SK_ID_BUREAU' column from the dataset as it doesn't add value\nbureau = bureau.drop(['SK_ID_BUREAU'], axis=1)\n\n## Average values for all other features in previous applications\nbureau_avg = bureau.groupby('SK_ID_CURR').mean()\nbureau_avg.columns = ['b_' + col for col in bureau_avg.columns]\ndata_train = data_train.merge(right=bureau_avg.reset_index(), how='left', on='SK_ID_CURR')",
      "execution_count": 98,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "b98aece8cd7d5a096157be491f57a1361c9f6206"
      },
      "cell_type": "code",
      "source": "if debugging:\n    display(bureau_avg.head())\n    display(bureau_avg.shape)",
      "execution_count": 99,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c9e7f293121fd26492be2e8d0334f5b6551e624",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Transforming skewed continuous features",
      "execution_count": 100,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "10916d50c700ec898123f3b422bb24064f2e4711"
      },
      "cell_type": "code",
      "source": "# Normalizing numerical features\n#from sklearn.preprocessing import MinMaxScaler",
      "execution_count": 101,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "95b3623001c0e8d93e84e3a9e6811264b7db861f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Turn categorical variables into numerical features using the one-hot encoding scheme\n\n# Total number of features. Excluding the load ids (SK_ID_CURR) and the target variable (TARGET)\nprint(\"Total number of features of train data, before one-hot encoding: {}\".format(data_train.shape[1] - 2))\n# Total number of features. Excluding the load ids (SK_ID_CURR). The test dataset has NO target label (TARGET)\nprint(\"Total number of features of test data, before one-hot encoding: {}\".format(data_test.shape[1] - 1))\n\n# One-hot encoding\ndata_train_encoded = pd.get_dummies(data_train)\ndata_test_encoded = pd.get_dummies(data_test)\n\n# Total number of features. Excluding the loan ids (SK_ID_CURR) and the target variable (TARGET)\nprint(\"Total number of features of train data, after one-hot encoding: {}\".format(data_train_encoded.shape[1] - 2))\n# Total number of features. Excluding the loan ids (SK_ID_CURR). The test dataset has NO target label (TARGET)\nprint(\"Total number of features of test data, after one-hot encoding: {}\".format(data_test_encoded.shape[1] - 1))\n\n# New list of features\n#print(list(data_train_encoded.columns))\n\n# Determine what columns are missing\ntrain_list = list(data_train_encoded.columns)\ntest_list = list(data_test_encoded.columns)\ndifference = [e for e in train_list if e not in test_list]\nprint(difference)\n\n# Add those columns to the test set will all zeros\ndata_test_encoded_complete = data_test_encoded\nfor e in difference:\n    if e != 'TARGET':\n        data_test_encoded_complete[e] = 0\n\nprint(\"Total number of features of test data, after one-hot encoding: {}\".format(data_test_encoded_complete.shape[1] - 1))",
      "execution_count": 102,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c29b942e0d37f6e2fc3249da9c88c9bbb2b0a84",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Separate the target label from the train dataset. The column we are interested is 'TARGET'. Name it target_train.\ntarget_train = data_train['TARGET']\nprint(\"Training target label has {:,} samples\".format(target_train.shape[0]))\n\n# Remove target label from the train dataset and rename to features_train.\nfeatures_train = data_train_encoded.drop(['TARGET'], axis=1)\n\n# Test data has no taget label 'TARGET' in the dataset\nfeatures_test = data_test_encoded_complete\n\n# Remove the column 'SK_ID_CURR' from the features_train and features_test dataset as it is not a feature\nfeatures_train = features_train.drop(['SK_ID_CURR'], axis=1)\nfeatures_test = features_test.drop(['SK_ID_CURR'], axis=1)",
      "execution_count": 103,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9f0fe582933f1a9086f982e0629a17ae1f9a266",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Handle missing values, alternative\n# https://pandas.pydata.org/pandas-docs/stable/missing_data.html\n# Previously used:\n# https://www.kaggle.com/dansbecker/handling-missing-values\n# http://scikit-learn.org/dev/modules/generated/sklearn.impute.SimpleImputer.html\n# But the result of that operation was a numpy.ndarray instead of a pandas.DataFrame\n\nprint(\"Total number of NaN in the training dataset before applying '.fillna(): {:,}\".format(features_train.isnull().sum().sum()))\nprint(\"Total number of NaN in the testing dataset before applying '.fillna(): {:,}\".format(features_train.isnull().sum().sum()))\n# Fill missing values with the column mean\nfeatures_train = features_train.fillna(features_train.mean())\nfeatures_test = features_test.fillna(features_test.mean())\nprint(\"Total number of NaN in the training dataset after applying '.fillna()': {:,}\".format(features_train.isnull().sum().sum()))\nprint(\"Total number of NaN in the testing dataset after applying '.fillna()': {:,}\".format(features_train.isnull().sum().sum()))",
      "execution_count": 104,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1de08efe6afcbc5ce61cc3711f9c250d07ec96c6",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Shuffle and split the data\n# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\nfrom sklearn.model_selection import train_test_split\n\n# Split the 'features' and 'target label' data into training and validating sets\nX_train, X_val, y_train, y_val = train_test_split(features_train,\n                                                  target_train,\n                                                  test_size=0.2,\n                                                  random_state=42)\n\nprint(\"Original Training set has {:,} samples.\".format(features_train.shape[0]))\nprint(\"After split Training set has {:,} samples.\".format(X_train.shape[0]))\nprint(\"After split Validating set has {:,} samples.\".format(X_val.shape[0]))\nprint(\"Testing set has {:,} samples.\".format(data_test.shape[0]))",
      "execution_count": 105,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9e96594845b10101ad32583f79665412b3994733",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8673e3d4c80bcae3d653f7d8b87f095381fcc0f2",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Just use application_train and application_test\n# Merge training and testing datasets\n# This will help in two ways:\n# When handling categorical variables it will ensure both datasets end up with the same features\n# When handling missing values, if we use the mean to fill in missing values, they will be more representative\napp_train['is_train'] = 1\napp_train['is_test'] = 0\napp_test['is_train'] = 0\napp_test['is_test'] = 1\n#display(app_train.head(3))\n#display(app_test.head(3))\nprint(\"\\nJoining the training and testing dataset for pre-processing.\")\ndata = pd.concat([app_train, app_test], axis=0, sort=False)\n# Substract 4 from the features count for the columns 'TARGET', 'SK_ID_CURR', 'is_train', 'is_test' for app_train\n# And substract 3 for app_test, as it doesn't have a 'TARGET' column\nprint(\"app_train has {0:,} samples and {1} features.\".format(app_train.shape[0], app_train.shape[1]-4))\nprint(\"app_test has {0:,} samples and {1} features.\".format(app_test.shape[0], app_test.shape[1]-3))\nprint(\"data has {0:,} samples and {1} features BEFORE one-hot encoding.\".format(data.shape[0], data.shape[1]-4))\nassert(data.shape[0] == app_train.shape[0] + app_test.shape[0])\nassert(data.shape[1] == max(app_train.shape[1], app_test.shape[1]))\n#display(data.head(3))\n#display(data[data['SK_ID_CURR'] == 100001])\n\n# Handle Categorical variables\n# http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html\nprint(\"\\nTransforming categorical variables into numerical features using the one-hot encoding scheme.\")\ndata = pd.get_dummies(data)\n# Substract 4 from the features count for the columns 'TARGET', 'SK_ID_CURR', 'is_train', 'is_test'\nprint(\"data has {0:,} samples and {1} features AFTER one-hot encoding.\".format(data.shape[0], data.shape[1]-4))\n#display(data.head(3))\n\n# Handle missing data\n# https://pandas.pydata.org/pandas-docs/stable/missing_data.html#filling-with-a-pandasobject\nprint(\"\\nFilling NaN values in the dataset using pandas.fillna() using the column mean() value.\")\nprint(\"Number of NaN values in the dataset BEFORE running pandas.fillna(): {:,}\".format(data.isnull().sum().sum()))\ndata = data.fillna(data.mean())\nnan_after = data.isnull().sum().sum()\nprint(\"Number of NaN values in the dataset AFTER running pandas.fillna(): {:,}\".format(nan_after))\nassert(nan_after == 0)\n\n# Separate the data into the original test and training datasets\n# Remove columns 'TARGET', 'SK_ID_CURR', 'is_train', 'is_test' as they are not features\nprint(\"\\nSeparating the training and testing dataset after completing pre-processing.\")\ntrain = data[data['is_train'] == 1]\n# Separate the target from the training dataset\ntarget = train['TARGET']\ntrain = train.drop(['TARGET', 'SK_ID_CURR', 'is_test', 'is_train'], axis=1)\ntest = data[data['is_test'] == 1]\ntest = test.drop(['TARGET', 'SK_ID_CURR', 'is_test', 'is_train'], axis=1)\nprint(\"train has {0:,} samples and {1} features.\".format(train.shape[0], train.shape[1]))\nprint(\"test has {0:,} samples and {1} features.\".format(test.shape[0], test.shape[1]))\n\n\n# Split into training and validation data\nprint(\"\\nSplitting the training dataset into actual training and validation datasets\")\nX_train, X_val, y_train, y_val = train_test_split(train, target, test_size=0.2, random_state=42)\nassert(train.shape[0] == X_train.shape[0] + X_val.shape[0])\nassert(X_train.shape[1] == train.shape[1])\nassert(X_val.shape[1] == train.shape[1])\nprint(\"training dataset has {0:,} samples and {1} features.\".format(X_train.shape[0], X_train.shape[1]))\nprint(\"validating dataset has {0:,} samples and {1} features.\".format(X_val.shape[0], X_val.shape[1]))",
      "execution_count": 106,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a950de15898a39888385bc2131ae8c8fdafadaa4",
        "_kg_hide-input": false,
        "_kg_hide-output": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# TEMPORARY SOLUTION - I NEED TO UNIFY NAMING\nfeatures_test = test.copy()",
      "execution_count": 112,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "51ba916b1d62b15d1d2fc1315a5f7647903e454f"
      },
      "cell_type": "code",
      "source": "# Run GridSearchCV or fully train an estimator\nrun_mode = 'train_estimator' # 'grid_search' or 'train_estimator'",
      "execution_count": 107,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d3362fd6c2b2cc65deaa4a18ecb49ed7a02d8263",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Run GridSearchCV\nif run_mode == 'grid_search':\n    perc_samples = 0.15\n    print(\"\\nPreparing to run Hyperparameters tunning with GridSearchCV using {0:.2f}% of the training samples\".format(perc_samples * 100))\n    features_train_small = X_train[:int(perc_samples * X_train.shape[0])]\n    target_train_small = y_train[:int(perc_samples * y_train.shape[0])]\n    features_val_small = X_val[:int(perc_samples * X_val.shape[0])]\n    target_val_small = y_val[:int(perc_samples * y_val.shape[0])]\n    #features_test_small = features_test[:int(perc_samples * features_test.shape[0])]\n\n    # Initialize the Estimator (Learner or Regression Model)\n    estimator = RandomForestRegressor(n_jobs=-1,\n                                      random_state=42,\n                                      verbose=0)\n\n    # Determine which Parameters to tune\n    '''\n    Tested so far:\n    parameters = {\n        'n_estimators': [9, 10, 11, 12, 13, 14, 15],\n        'criterion': ['mse', 'mae'],\n        'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7],\n        'max_features': [0.01, 0.1, 0.25, 0.45, 0.5, 0.55, 0.6, 0.75],\n        'min_samples_split': [2, 3, 4, 5],\n        'warm_start': [False, True]\n    }\n    '''\n    parameters = {\n        'n_estimators': [130, 135, 145],\n        'min_samples_leaf': [55, 62, 75],\n        'max_features': [0.2], # [0.18, 0.2, 0.23]\n        'min_samples_split': [2], # [2, 3]\n    }\n\n    # Create a scorer to measure hyperparameters performance\n    scorer = make_scorer(roc_auc_score)\n\n    # Create GridSearchCV grid object\n    grid_obj = GridSearchCV(estimator=estimator, \n                            param_grid=parameters, \n                            scoring=scorer)\n\n    # Fit the GridSearchCV grid object with the reduced training dataset and find the best hyperparameters\n    start = time()\n    grid_fit = grid_obj.fit(features_train_small, target_train_small)\n    end = time()\n    grid_fit_time = (end - start) / 60 # Ellapsed time in minutes\n    print(\"\\nGridSearchCV estimator fit time: {0:.2f} minutes\".format((end - start) / 60))\n\n    # Get the best estimator\n    best_est = grid_obj.best_estimator_\n    print(\"\\nBest Estimator: \\n{}\\n\".format(best_est))\n\n    # Get the best score\n    best_score = grid_obj.best_score_\n    print(\"\\nBest Estimator Score: {}\\n\".format(best_score))\n\n    # Get the best parameters\n    best_params = grid_obj.best_params_\n    print(\"\\nBest Hyperparameters that yield the best score: \\n{}\\n\".format(best_params))\n\n    # Make predictions with unoptimized estimator on the validation set\n    pred_val = (estimator.fit(features_train_small, target_train_small)).predict(features_val_small)\n    print(\"\\nUnoptimized Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(target_val_small, pred_val)))\n\n    # Predict with the best estimator on the validation set\n    best_pred_val = best_est.predict(features_val_small)\n    print(\"\\nOptimized Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(target_val_small, best_pred_val)))\n\n    # Predict with the best estimator on the testing set\n    #pred_test = best_est.predict(features_test)",
      "execution_count": 108,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5590b6b3ffee50b6734fb9f81a85b72b7b1e33a3",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Train estimator LGBM\nimport lightgbm as lgb\n\nif run_mode == 'TEMP_REPLACED':\n    lgb_train = lgb.Dataset(data=X_train, label=y_train)\n    lgb_eval = lgb.Dataset(data=X_val, label=y_val)\n\n    params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n              'learning_rate': 0.01, 'num_leaves': 48, 'num_iteration': 5000, 'verbose': 0 ,\n              'colsample_bytree':.8, 'subsample':.9, 'max_depth':7, 'reg_alpha':.1, 'reg_lambda':.1, \n              'min_split_gain':.01, 'min_child_weight':1}\n\n    model = lgb.train(params, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=150, verbose_eval=200)\n    print(\"\\nPreparing to train the following estimator: \\n{}\".format(model))\n\n    lgb.plot_importance(model, figsize=(12, 25), max_num_features=100);\n    \n    # Predict using the 'test' dataset for submission\n    pred_test = model.predict(features_test)\n    \n    # Prepare prediction for submission\n    submission = pd.DataFrame()\n    # Need to replace data_test_encoded_complete\n    submission['SK_ID_CURR'] = data_test_encoded_complete['SK_ID_CURR']\n    submission['TARGET'] = pred_test\n    submission.head()\n    submission.to_csv('RFR.csv', index=False)",
      "execution_count": 109,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "adf25ba4e24d459ac9e0fe1631a73609a6989e99",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Train estimator RandonForrestRegressor\nif run_mode == 'train_estimator':\n    # Initialize the Estimator (Learner or Regression Model) with the best hyperparameters\n    estimator = RandomForestRegressor(n_estimators=125, # default=10\n                                      max_features=0.2, # default='auto'\n                                      min_samples_split=2, # default=2\n                                      min_samples_leaf=75, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=0) # default=0\n    print(\"\\nPreparing to train the following estimator: \\n{}\".format(estimator))\n\n    # Fit the estimator with the training dataset\n    start = time()\n    estimator.fit(X_train, y_train)\n    end = time()\n    print(\"\\nEstimator fit time: {} seconds\".format(end - start))\n\n    # Predict with the validation dataset\n    pred_val = estimator.predict(X_val)\n    print(\"\\nEstimator prediction score on Validation set: \\t{}\".format(roc_auc_score(y_val, pred_val)))\n    \n    # Determine the feature importance\n    fi = pd.DataFrame()\n    fi['feature'] = X_train.columns\n    fi['importance'] = estimator.feature_importances_\n    display(fi.sort_values(by=['importance'], ascending=False).head(10))\n\n    # TODO: GRAPH THE FEATURE IMPORTANCE\n    \n    # Predict using the 'test' dataset for submission\n    pred_test = estimator.predict(features_test)\n    \n    # Prepare prediction for submission\n    print(\"\\nPreparing prediction for submission.\")\n    submission = pd.DataFrame()\n    # Need to replace data_test_encoded_complete\n    submission['SK_ID_CURR'] = data_test_encoded_complete['SK_ID_CURR']\n    submission['TARGET'] = pred_test\n    submission.head()\n    submission.to_csv('RFR.csv', index=False)",
      "execution_count": 113,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06daaf465dc5e34eab324a1ecd7b3c80183b73ed",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Train estimator\n# TODO: rename or remove '_small', it might be '_full' or nothing\nif run_mode == 'DISABLED':\n    # Use the full training and validation datasets to fit the estimator with the best hyperparameters\n    perc_samples = 1\n    print(\"Preparing to train an estimator using {0:.2f}% of the training dataset\".format(perc_samples * 100))\n    features_train_small = X_train[:int(perc_samples * X_train.shape[0])]\n    target_train_small = y_train[:int(perc_samples * y_train.shape[0])]\n    features_val_small = X_val[:int(perc_samples * X_val.shape[0])]\n    target_val_small = y_val[:int(perc_samples * y_val.shape[0])]\n    features_test_small = features_test[:int(perc_samples * features_test.shape[0])]\n\n    # Initialize the Estimator (Learner or Regression Model) with the best hyperparameters\n    '''\n    estimator = RandomForestRegressor(criterion='mae', # default='mse', VERY SLOW\n                                      min_samples_split=2, # default=2\n                                      warm_start=False) # default=False\n    Best Performance:\n    estimator = RandomForestRegressor(n_estimators=135, # default=10\n                                      max_features=0.2, # default='auto'\n                                      min_samples_split=2, # default=2\n                                      min_samples_leaf=62, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=2) # default=0\n                                      ROC_AUC_SCORE:  0.7289233660986658 (% samples: 0.1, Data: ALL)\n                                      LEADERBOARD SCORE: \n\n    estimator = RandomForestRegressor(n_estimators=125, # default=10\n                                      max_features=0.2, # default='auto'\n                                      min_samples_split=2, # default=2\n                                      min_samples_leaf=75, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=2) # default=0\n                                      ROC_AUC_SCORE:  0.7417496532130599 (% samples: 0.15)\n                                      LEADERBOARD SCORE: 0.722 (NO improvement)\n\n    estimator = RandomForestRegressor(n_estimators=100, # default=10\n                                      max_features=0.2, # default='auto'\n                                      min_samples_split=3, # default=2\n                                      min_samples_leaf=50, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=2) # default=0\n                                      ROC_AUC_SCORE:  0.7286786065442892 (% samples: 0.1)\n                                      LEADERBOARD SCORE: 0.722\n\n    estimator = RandomForestRegressor(n_estimators=12, # default=10\n                                      max_features=0.45, # default='auto'\n                                      min_samples_leaf=2, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=2) # default=0\n                                      LEADERBOARD SCORE: 0.62\n    '''\n    estimator = RandomForestRegressor(n_estimators=135, # default=10\n                                      max_features=0.2, # default='auto'\n                                      min_samples_split=2, # default=2\n                                      min_samples_leaf=62, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=0) # default=0\n\n    # Fit the estimator with the training dataset\n    start = time()\n    estimator.fit(features_train_small, target_train_small)\n    end = time()\n    print(\"Estimator fit time: {0:.2f} minutes\".format((end - start) / 60))\n\n    # Predict with the validation dataset\n    pred_val = estimator.predict(features_val_small)\n    print(\"Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(target_val_small, pred_val)))\n    \n    # Predict using the 'test' dataset for submission\n    pred_test = estimator.predict(features_test_small)\n    #pred_test = estimator.predict(features_test)\n\n    # Prepare prediction for submission\n    submission = pd.DataFrame()\n    # Need to replace data_test_encoded_complete\n    submission['SK_ID_CURR'] = data_test_encoded_complete['SK_ID_CURR'][:int(perc_samples * features_test.shape[0])]\n    submission['TARGET'] = pred_test\n    submission.head()\n    submission.to_csv('RFR.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "21226bcc68d399a572f5a950b1cb6ba5a47c542f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "85c8c3c19174276cdc1ee4adca42d90b059479ad"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}