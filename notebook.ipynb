{
  "cells": [
    {
      "metadata": {
        "_uuid": "a50ca0e6adc3396caaee0109fd82fee783d1073d"
      },
      "cell_type": "markdown",
      "source": "June 7 through June 17\nFuture steps - Must do:\n* Using the other data files (Feature Engineering, Create flat dataset)\n* Data visualization\n* Normalizing data\n* Handling skewed data\nReport:\n* Preliminary results\n\nJune 18 through July 1\nReport:\n* WEEK1: I should have a skeleton of all the areas of the report to share with mentor.\n* WEEK2: I should have a first full version of the report to share with mentor and ready to submit\nFuture steps - Nice to do:\n* CREATE A scikit-learn Pipeline\n* MAKE AN ENSEMBLE METHOD OF MULTIPLE MODELS\n* Measuring model/learner performance using other evaluation metric (LR+, f1, r2)?\n* Hyperparameter tunening (different values until I find a max, use higher percentage of training & validation data)\n* Review Handling missing or invalid data, other better methods?\n\nDONE:\n* Handling missing or invalid data\n* Split train & validation set\n* Hyperparameter tunening\n* Measuring model/learner performance using evaluation metric (auc_roc_score)"
    },
    {
      "metadata": {
        "_uuid": "d355f28e4f031ceb02bb7223ae79f96353cff7c8"
      },
      "cell_type": "markdown",
      "source": "Preliminary results:\n- Categorical values turned into numerical features with one-hot encoding scheme\n- Fill missing or wrong values\n- RFR Default values, only application training data, score: 0.591\n- RFR Best values with GridSearchCV, only application training data, score: 0.722 <- 0.62\n- RFR Best values with GridSearchCV, ALL available data, score: ?\n- ???? Best values with GridSearchCV, ALL available data, score: ?\n- Normalizing Data\n- Unskewing Data\n\nAbbreviations:\nRFR: Random Forest Regressor\n????: Another regressor"
    },
    {
      "metadata": {
        "_uuid": "9615588ff15ee992f488696fa0a5fa1394bb2707"
      },
      "cell_type": "markdown",
      "source": "# **Sections:**\n[1. Import libraries & support functions](#1)  \n[2. Dataset preparation](#2)  \n[3. Exploratory Data Analysis (EDA)](#3)  \n&nbsp; [3.1 Application Train](#3.1)  \n[4. Feature Engineering](#4)  \n&nbsp; [4.1 Handle Skewed Continuous Data](#4.1)  \n&nbsp; [4.2 Normalize Continuous Data](#4.2)  \n&nbsp; [4.3 Handle Categorical Variables](#4.3)  \n&nbsp; [4.4 Handle Missing Data](#4.4)  \n&nbsp; [4.5 Create a Flat Dataset](#4.5)  \n[5. Split Data into Training and Validation](#5)  \n[6. Hyperparameter Tuning](#6)  \n[7. Model Fitting & Prediction](#7)  "
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSVfile I/O (e.g. pd.read_csv)\nimport os\nfrom plotly.offline import init_notebook_mode, iplot\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom plotly import tools\n# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\nfrom sklearn.ensemble import RandomForestRegressor\n# Add evaluation metric to measure the model's performance\n# Regression metrics available:\n# http://scikit-learn.org/stable/modules/classes.html#regression-metrics\n# http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\n# http://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc\n# http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html\n# Cannot use sklearn.metrics.accuracy_score as it is a Classification metric\nfrom sklearn.metrics import make_scorer, r2_score, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom time import time\n\n\n'''\ndef bar_hor(df, col, title, color, w=None, h=None, lm=0, limit=100, return_trace=False, rev=False, xlb = False):\n    cnt_srs = df[col].value_counts()\n    yy = cnt_srs.head(limit).index[::-1] \n    xx = cnt_srs.head(limit).values[::-1] \n    if rev:\n        yy = cnt_srs.tail(limit).index[::-1] \n        xx = cnt_srs.tail(limit).values[::-1] \n    if xlb:\n        trace = go.Bar(y=xlb, x=xx, orientation = 'h', marker=dict(color=color))\n    else:\n        trace = go.Bar(y=yy, x=xx, orientation = 'h', marker=dict(color=color))\n    if return_trace:\n        return trace \n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n\ndef gp(col, title):\n    df1 = data_train[data_train[\"TARGET\"] == 1]\n    df0 = data_train[data_train[\"TARGET\"] == 0]\n    a1 = df1[col].value_counts()\n    b1 = df0[col].value_counts()\n    \n    total = dict(data_train[col].value_counts())\n    x0 = a1.index\n    x1 = b1.index\n    \n    y0 = [float(x)*100 / total[x0[i]] for i,x in enumerate(a1.values)]\n    y1 = [float(x)*100 / total[x1[i]] for i,x in enumerate(b1.values)]\n\n    trace1 = go.Bar(x=a1.index, y=y0, name='Target : 1', marker=dict(color=\"#96D38C\"))\n    trace2 = go.Bar(x=b1.index, y=y1, name='Target : 0', marker=dict(color=\"#FEBFB3\"))\n    return trace1, trace2 \n'''\n\nprint(os.listdir(\"../input\"))",
      "execution_count": 58,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "_kg_hide-input": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from IPython.display import display # Allows the use of display() for DataFrames\n\n# Data exploration\n\n# Load the application training dataset application_train.csv\ndata_train = pd.read_csv(\"../input/application_train.csv\")\n\n# Load the application testing dataset application_test.csv\ndata_test = pd.read_csv(\"../input/application_test.csv\")\n\n'''\n# Display the first 5 records of application_train.csv\ndisplay(data_train.head())\n\n# Display the first 5 records of application_test.csv\ndisplay(data_test.head())\n'''\n\n# DataFrame statistics summary for selected columns\ndata_train[[\"AMT_INCOME_TOTAL\", \"AMT_CREDIT\", \"AMT_ANNUITY\", \"AMT_GOODS_PRICE\"]].describe()\n#print(len(data_train.columns))",
      "execution_count": 59,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3fe5a388f860fb1ebdc3698221db4e0c66697726",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Total number of records\nprint(\"Total number of records in the train dataset: {:,}\".format(len(data_train)))\n# Total number of features. Excluding the load ids (SK_ID_CURR) and the target variable (TARGET).\nprint(\"Total number of features in the train dataset: {}\".format(data_train.shape[1] - 2))\n\n# Total number of records\nprint(\"Total number of records in the test dataset: {:,}\".format(len(data_test)))\n# Total number of features. Excluding the load ids (SK_ID_CURR). There is NO target variable (TARGET) in the test dataset.\nprint(\"Total number of features in the test dataset: {}\".format(data_test.shape[1] - 1))",
      "execution_count": 60,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ee2f2a92f145a72cccd670372cd2cd6fefd0dca",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# GRAPHS ARE NOT DISPLAYING\n'''\n# Target Variable Distribution \nbar_hor(data_train, \"TARGET\", \"Distribution of Target Variable\" , [\"#44ff54\", '#ff4444'], h=350, w=600, lm=200, xlb = ['Target : 1','Target : 0'])\n\ntr0 = bar_hor(data_train, \"CODE_GENDER\", \"Distribution of CODE_GENDER Variable\" ,\"#f975ae\", w=700, lm=100, return_trace= True)\ntr1, tr2 = gp('CODE_GENDER', 'Distribution of Target with Applicant Gender')\n\nfig = tools.make_subplots(rows=1, cols=3, print_grid=False, subplot_titles = [\"Gender Distribution\" , \"Gender, Target=1\" ,\"Gender, Target=0\"])\nfig.append_trace(tr0, 1, 1);\nfig.append_trace(tr1, 1, 2);\nfig.append_trace(tr2, 1, 3);\nfig['layout'].update(height=350, showlegend=False, margin=dict(l=50));\niplot(fig);\n'''",
      "execution_count": 61,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c9e7f293121fd26492be2e8d0334f5b6551e624",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Transforming skewed continuous features",
      "execution_count": 62,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "10916d50c700ec898123f3b422bb24064f2e4711"
      },
      "cell_type": "code",
      "source": "# Normalizing numerical features\nfrom sklearn.preprocessing import MinMaxScaler",
      "execution_count": 63,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "95b3623001c0e8d93e84e3a9e6811264b7db861f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Turn categorical variables into numerical features using the one-hot encoding scheme\n\n# Total number of features. Excluding the load ids (SK_ID_CURR) and the target variable (TARGET)\nprint(\"Total number of features of train data, before one-hot encoding: {}\".format(data_train.shape[1] - 2))\n# Total number of features. Excluding the load ids (SK_ID_CURR). The test dataset has NO target label (TARGET)\nprint(\"Total number of features of test data, before one-hot encoding: {}\".format(data_test.shape[1] - 1))\n\n# One-hot encoding\ndata_train_encoded = pd.get_dummies(data_train)\ndata_test_encoded = pd.get_dummies(data_test)\n\n# Total number of features. Excluding the load ids (SK_ID_CURR) and the target variable (TARGET)\nprint(\"Total number of features of train data, after one-hot encoding: {}\".format(data_train_encoded.shape[1] - 2))\n# Total number of features. Excluding the load ids (SK_ID_CURR). The test dataset has NO target label (TARGET)\nprint(\"Total number of features of test data, after one-hot encoding: {}\".format(data_test_encoded.shape[1] - 1))\n\n# New list of features\n#print(list(data_train_encoded.columns))\n\n# Determine what columns are missing\ntrain_list = list(data_train_encoded.columns)\ntest_list = list(data_test_encoded.columns)\ndifference = [e for e in train_list if e not in test_list]\nprint(difference)\n\n# Add those columns to the test set will all zeros\ndata_test_encoded_complete = data_test_encoded\nfor e in difference:\n    if e != 'TARGET':\n        data_test_encoded_complete[e] = 0\n\nprint(\"Total number of features of test data, after one-hot encoding: {}\".format(data_test_encoded_complete.shape[1] - 1))",
      "execution_count": 64,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c29b942e0d37f6e2fc3249da9c88c9bbb2b0a84",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Separate the target label from the train dataset. The column we are interested is 'TARGET'. Name it target_train.\ntarget_train = data_train['TARGET']\nprint(\"Training target label has {:,} samples\".format(target_train.shape[0]))\n\n# Remove target label from the train dataset and rename to features_train.\nfeatures_train = data_train_encoded.drop(['TARGET'], axis=1)\n\n# Test data has no taget label 'TARGET' in the dataset\nfeatures_test = data_test_encoded_complete",
      "execution_count": 65,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "70653fe98cc7a8c16117c897ecc2308223fdbbb0",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Handle missing values\n# Source: https://www.kaggle.com/dansbecker/handling-missing-values\n# http://scikit-learn.org/dev/modules/generated/sklearn.impute.SimpleImputer.html\n\nfrom sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\nfeatures_train = my_imputer.fit_transform(features_train)\nfeatures_test = my_imputer.fit_transform(features_test)",
      "execution_count": 66,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1de08efe6afcbc5ce61cc3711f9c250d07ec96c6",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Shuffle and split the data\n# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\nfrom sklearn.model_selection import train_test_split\n\n# Split the 'features' and 'target label' data into training and validating sets\nX_train, X_val, y_train, y_val = train_test_split(features_train,\n                                                  target_train,\n                                                  test_size=0.2,\n                                                  random_state=42)\n\nprint(\"Original Training set has {:,} samples.\".format(features_train.shape[0]))\nprint(\"After split Training set has {:,} samples.\".format(X_train.shape[0]))\nprint(\"After split Validating set has {:,} samples.\".format(X_val.shape[0]))\nprint(\"Testing set has {:,} samples.\".format(data_test.shape[0]))",
      "execution_count": 67,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "51ba916b1d62b15d1d2fc1315a5f7647903e454f"
      },
      "cell_type": "code",
      "source": "# Run GridSearchCV or fully train an estimator\nrun_mode = 'grid_search' # 'grid_search' or 'train_estimator'",
      "execution_count": 68,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d3362fd6c2b2cc65deaa4a18ecb49ed7a02d8263",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Run GridSearchCV\nif run_mode == 'grid_search':\n    perc_samples = 0.001\n    print(\"Preparing to run Hyperparameters tunning with GridSearchCV using {0:.2f}% of the training samples\".format(perc_samples * 100))\n    features_train_small = X_train[:int(perc_samples * X_train.shape[0])]\n    target_train_small = y_train[:int(perc_samples * y_train.shape[0])]\n    features_val_small = X_val[:int(perc_samples * X_val.shape[0])]\n    target_val_small = y_val[:int(perc_samples * y_val.shape[0])]\n    #features_test_small = features_test[:int(perc_samples * features_test.shape[0])]\n\n    # Initialize the Estimator (Learner or Regression Model)\n    estimator = RandomForestRegressor(n_jobs=-1,\n                                      random_state=42,\n                                      verbose=0)\n\n    # Determine which Parameters to tune\n    '''\n    Tested so far:\n    parameters = {\n        'n_estimators': [9, 10, 11, 12, 13, 14, 15],\n        'criterion': ['mse', 'mae'],\n        'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7],\n        'max_features': [0.01, 0.1, 0.25, 0.45, 0.5, 0.55, 0.6, 0.75],\n        'min_samples_split': [2, 3, 4, 5],\n        'warm_start': [False, True]\n    }\n    '''\n    parameters = {\n        'n_estimators': [115, 125, 135],\n        'min_samples_leaf': [62, 75, 87],\n        'max_features': [0.18, 0.2, 0.23],\n        'min_samples_split': [2, 3],\n    }\n\n    # Create a scorer to measure hyperparameters performance\n    scorer = make_scorer(roc_auc_score)\n\n    # Create GridSearchCV grid object\n    grid_obj = GridSearchCV(estimator=estimator, \n                            param_grid=parameters, \n                            scoring=scorer)\n\n    # Fit the GridSearchCV grid object with the reduced training dataset and find the best hyperparameters\n    start = time()\n    grid_fit = grid_obj.fit(features_train_small, target_train_small)\n    end = time()\n    grid_fit_time = (end - start) / 60 # Ellapsed time in minutes\n    print(\"GridSearchCV estimator fit time: {0:.2f} minutes\".format((end - start) / 60))\n\n    # Get the best estimator\n    best_est = grid_obj.best_estimator_\n    print(\"Best Estimator: \\n{}\\n\".format(best_est))\n\n    # Get the best score\n    best_score = grid_obj.best_score_\n    print(\"Best Estimator Score: {}\\n\".format(best_score))\n\n    # Get the best parameters\n    best_params = grid_obj.best_params_\n    print(\"Best Hyperparameters that yield the best score: \\n{}\\n\".format(best_params))\n\n    # Make predictions with unoptimized estimator on the validation set\n    pred_val = (estimator.fit(features_train_small, target_train_small)).predict(features_val_small)\n    print(\"Unoptimized Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(target_val_small, pred_val)))\n\n    # Predict with the best estimator on the validation set\n    best_pred_val = best_est.predict(features_val_small)\n    print(\"Optimized Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(target_val_small, best_pred_val)))\n\n    # Predict with the best estimator on the testing set\n    #pred_test = best_est.predict(features_test)",
      "execution_count": 72,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06daaf465dc5e34eab324a1ecd7b3c80183b73ed",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Train estimator\n# TODO: rename or remove '_small', it might be '_full' or nothing\nif run_mode == 'train_estimator':\n    # Use the full training and validation datasets to fit the estimator with the best hyperparameters\n    perc_samples = 1\n    print(\"Preparing to train an estimator using {0:.2f}% of the training dataset\".format(perc_samples * 100))\n    features_train_small = X_train[:int(perc_samples * X_train.shape[0])]\n    target_train_small = y_train[:int(perc_samples * y_train.shape[0])]\n    features_val_small = X_val[:int(perc_samples * X_val.shape[0])]\n    target_val_small = y_val[:int(perc_samples * y_val.shape[0])]\n    features_test_small = features_test[:int(perc_samples * features_test.shape[0])]\n\n    # Initialize the Estimator (Learner or Regression Model) with the best hyperparameters\n    '''\n    estimator = RandomForestRegressor(criterion='mae', # default='mse', VERY SLOW\n                                      min_samples_split=2, # default=2\n                                      warm_start=False) # default=False\n    Best Performance:\n    estimator = RandomForestRegressor(n_estimators=125, # default=10\n                                      max_features=0.2, # default='auto'\n                                      min_samples_split=2, # default=2\n                                      min_samples_leaf=75, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=2) # default=0\n                                      ROC_AUC_SCORE:  0.7417496532130599 (% samples: 0.15)\n                                      LEADERBOARD SCORE: 0.722 (NO improvement)\n\n    estimator = RandomForestRegressor(n_estimators=100, # default=10\n                                      max_features=0.2, # default='auto'\n                                      min_samples_split=3, # default=2\n                                      min_samples_leaf=50, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=2) # default=0\n                                      ROC_AUC_SCORE:  0.7286786065442892 (% samples: 0.1)\n                                      LEADERBOARD SCORE: 0.722\n\n    estimator = RandomForestRegressor(n_estimators=12, # default=10\n                                      max_features=0.45, # default='auto'\n                                      min_samples_leaf=2, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=2) # default=0\n                                      LEADERBOARD SCORE: 0.62\n    '''\n    estimator = RandomForestRegressor(n_estimators=125, # default=10\n                                      max_features=0.2, # default='auto'\n                                      min_samples_split=2, # default=2\n                                      min_samples_leaf=75, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=0) # default=0\n\n    # Fit the estimator with the training dataset\n    start = time()\n    estimator.fit(features_train_small, target_train_small)\n    end = time()\n    print(\"Estimator fit time: {0:.2f} minutes\".format((end - start) / 60))\n\n    # Predict with the validation dataset\n    pred_val = estimator.predict(features_val_small)\n    print(\"Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(target_val_small, pred_val)))\n    \n    # Predict using the 'test' dataset for submission\n    pred_test = estimator.predict(features_test_small)\n    #pred_test = estimator.predict(features_test)\n\n    # Prepare prediction for submission\n    submission = pd.DataFrame()\n    # Need to replace data_test_encoded_complete\n    submission['SK_ID_CURR'] = data_test_encoded_complete['SK_ID_CURR'][:int(perc_samples * features_test.shape[0])]\n    submission['TARGET'] = pred_test\n    submission.head()\n    submission.to_csv('RFR.csv', index=False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}